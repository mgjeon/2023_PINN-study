{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import solve_bvp\n",
    "\n",
    "\n",
    "def _differential_equation(mu, u, n, a2):\n",
    "    \"\"\"\n",
    "    The differential equation to solve for P\n",
    "\n",
    "    :param mu: cos(theta)\n",
    "    :param u: P function and derivative\n",
    "    :param n: variable according to Low & Lou (1989)\n",
    "    :param a2: eigenvalue\n",
    "\n",
    "    \"\"\"\n",
    "    P, dP = u\n",
    "    dP_dmu = dP\n",
    "    d2P_dmu2 = -(n * (n + 1) * P + a2 * (1 + n) / n * P ** (1 + 2 / n)) / (1 - mu ** 2 + 1e-8)\n",
    "    return (dP_dmu, d2P_dmu2)\n",
    "\n",
    "\n",
    "def get_analytic_b_field(n=1, m=1, l=0.3, psi=np.pi / 4, resolution=64, bounds=[-1, 1, -1, 1, 0, 2]):\n",
    "    \"\"\"\n",
    "    Calculate the analytic NLFF field from Low & Lou (1989).\n",
    "\n",
    "    :param n: variable see Low & Lou (1989), only works for n=1\n",
    "    :param m: used for generating a proper initial condition.\n",
    "    :param a2: eigenvalue\n",
    "    :param l: depth below the photosphere\n",
    "    :param psi: angle of the magnetic field relative to the dipol axis\n",
    "    :param resolution: spatial resolution of the magnetic field in pixels\n",
    "    :param bounds: dimensions of the volume (x_start, x_end, y_start, y_end, z_start, z_end)\n",
    "    :return: magnetic field B (x, y, z, v)\n",
    "    \"\"\"\n",
    "    sol_P, a2 = solve_P(n, m)\n",
    "\n",
    "    resolution = [resolution] * 3 if not isinstance(resolution, list) else resolution\n",
    "    coords = np.stack(np.meshgrid(np.linspace(bounds[0], bounds[1], resolution[1], dtype=np.float32),\n",
    "                                  np.linspace(bounds[2], bounds[3], resolution[0], dtype=np.float32),\n",
    "                                  np.linspace(bounds[4], bounds[5], resolution[2], dtype=np.float32)), -1).transpose(\n",
    "        [1, 0, 2, 3])\n",
    "\n",
    "    x, y, z = coords[..., 0], coords[..., 1], coords[..., 2]\n",
    "    X = x * np.cos(psi) - (z + l) * np.sin(psi)\n",
    "    Y = y\n",
    "    Z = x * np.sin(psi) + (z + l) * np.cos(psi)\n",
    "\n",
    "    # to spherical coordinates\n",
    "    xy = X ** 2 + Y ** 2\n",
    "    r = np.sqrt(xy + Z ** 2)\n",
    "    theta = np.arctan2(np.sqrt(xy), Z)\n",
    "    phi = np.arctan2(Y, X)\n",
    "\n",
    "    mu = np.cos(theta)\n",
    "\n",
    "    P, dP_dmu = sol_P(mu)\n",
    "    A = P / r ** n\n",
    "    dA_dtheta = -np.sin(theta) / (r ** n) * dP_dmu\n",
    "    dA_dr = P * (-n * r ** (-n - 1))\n",
    "    Q = np.sqrt(a2) * A * np.abs(A) ** (1 / n)\n",
    "\n",
    "    Br = (r ** 2 * np.sin(theta)) ** -1 * dA_dtheta\n",
    "    Btheta = - (r * np.sin(theta)) ** -1 * dA_dr\n",
    "    Bphi = (r * np.sin(theta)) ** -1 * Q\n",
    "\n",
    "    BX = Br * np.sin(theta) * np.cos(phi) + Btheta * np.cos(theta) * np.cos(phi) - Bphi * np.sin(phi)\n",
    "    BY = Br * np.sin(theta) * np.sin(phi) + Btheta * np.cos(theta) * np.sin(phi) + Bphi * np.cos(phi)\n",
    "    BZ = Br * np.cos(theta) - Btheta * np.sin(theta)\n",
    "\n",
    "    Bx = BX * np.cos(psi) + BZ * np.sin(psi)\n",
    "    By = BY\n",
    "    Bz = - BX * np.sin(psi) + BZ * np.cos(psi)\n",
    "\n",
    "    b_field = np.real(np.stack([Bx, By, Bz], -1))\n",
    "    return b_field\n",
    "\n",
    "\n",
    "def solve_P(n, m):\n",
    "    \"\"\"\n",
    "    Solve the differential equation from Low & Lou (1989).\n",
    "\n",
    "    :param n: variable (only n=1)\n",
    "    :param v0: start condition for dP/dmu\n",
    "    :param P0: boundary condition for P(-1) and P(1)\n",
    "    :return: interpolated functions for P and dP/dmu\n",
    "    \"\"\"\n",
    "\n",
    "    def f(x, y, p):\n",
    "        a2 = p[0]\n",
    "        d2P_dmu2 = -(n * (n + 1) * y[0] + a2 * (1 + n) / n * y[0] ** (1 + 2 / n)) / (1 - x ** 2 + 1e-6)\n",
    "        return [y[1], d2P_dmu2]\n",
    "\n",
    "    def f_boundary(Pa, Pb, p):\n",
    "        return np.array([Pa[0] - 0, Pb[0] - 0, Pa[1] - 10])\n",
    "\n",
    "    mu = np.linspace(-1, 1, num=256)\n",
    "\n",
    "    if m % 2 == 0:\n",
    "        init = np.cos(mu * (m + 1) * np.pi / 2)\n",
    "    else:\n",
    "        init = np.sin(mu * (m + 1) * np.pi / 2)\n",
    "\n",
    "    dinit = 10 * np.ones_like(init)  #\n",
    "    initial = np.stack([init, dinit])\n",
    "\n",
    "    @np.vectorize\n",
    "    def shooting(a2_init):\n",
    "        eval = solve_bvp(f, f_boundary, x=mu, y=initial, p=[a2_init], verbose=0, tol=1e-6)\n",
    "        if eval.success == False:\n",
    "            return None\n",
    "        return eval\n",
    "\n",
    "    # use shooting to find eigenvalues\n",
    "    evals = shooting(np.linspace(0, 10, 100, dtype=np.float32))\n",
    "    evals = [e for e in evals if e is not None]\n",
    "\n",
    "    eigenvalues = np.array([e.p for e in evals])\n",
    "    eigenvalues = sorted(set(np.round(eigenvalues, 4).reshape((-1,))))\n",
    "\n",
    "    # get final solution\n",
    "    eval = shooting([eigenvalues[-1]])[0]\n",
    "\n",
    "    return eval.sol, eval.p[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os \n",
    "\n",
    "config_path = 'config_run.json'\n",
    "\n",
    "with open(config_path) as config:\n",
    "    info = json.load(config)\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= info['simul']['gpu_id']\n",
    "\n",
    "n = info['exact']['n']\n",
    "m = info['exact']['m']\n",
    "l = info['exact']['l']\n",
    "psi = eval(info['exact']['psi'])\n",
    "resolution = info['exact']['resolution']\n",
    "bounds = info['exact']['bounds']\n",
    "\n",
    "B = get_analytic_b_field(n=n, m=m, l=l, psi=psi, resolution=resolution, bounds=bounds)\n",
    "\n",
    "base_path = os.path.join(info['simul']['base_path'], \"run/\")\n",
    "meta_path = info['simul']['meta_path']\n",
    "\n",
    "bin = info['simul']['bin']\n",
    "\n",
    "height = info['simul']['height']\n",
    "spatial_norm = info['simul']['spatial_norm']\n",
    "b_norm = info['simul']['b_norm']\n",
    "\n",
    "meta_info = info['simul']['meta_info']\n",
    "dim = info['simul']['dim']\n",
    "positional_encoding = info['simul']['positional_encoding']\n",
    "use_potential_boundary = info['simul']['use_potential_boundary']\n",
    "potential_strides = info['simul']['potential_strides']\n",
    "use_vector_potential = info['simul']['use_vector_potential']\n",
    "w_div = info['simul']['w_div']\n",
    "w_ff = info['simul']['w_ff']\n",
    "decay_iterations = info['simul']['decay_iterations']\n",
    "device = info['simul']['device']\n",
    "work_directory = info['simul']['work_directory']\n",
    "\n",
    "total_iterations = info['simul']['total_iterations']\n",
    "batch_size = info['simul']['batch_size']\n",
    "log_interval = info['simul']['log_interval']\n",
    "validation_interval = info['simul']['validation_interval']\n",
    "num_workers = info['simul']['num_workers']\n",
    "\n",
    "\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "b_bottom = B[:, :, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda import get_device_name\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "from astropy.nddata import block_reduce\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, RandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian(output, coords):\n",
    "    jac_matrix = [torch.autograd.grad(output[:, i], coords,\n",
    "                                      grad_outputs=torch.ones_like(output[:, i]).to(output),\n",
    "                                      retain_graph=True,\n",
    "                                      create_graph=True)[0]\n",
    "                  for i in range(output.shape[1])]\n",
    "    jac_matrix = torch.stack(jac_matrix, dim=1)\n",
    "    return jac_matrix\n",
    "\n",
    "class Sine(nn.Module):\n",
    "    def __init__(self, w0=1.):\n",
    "        super().__init__()\n",
    "        self.w0 = w0\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(self.w0 * x)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional Encoding of the input coordinates.\n",
    "\n",
    "    encodes x to (..., sin(2^k x), cos(2^k x), ...)\n",
    "    k takes \"num_freqs\" number of values equally spaced between [0, max_freq]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_freq, num_freqs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_freq (int): maximum frequency in the positional encoding.\n",
    "            num_freqs (int): number of frequencies between [0, max_freq]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        freqs = 2 ** torch.linspace(0, max_freq, num_freqs)\n",
    "        self.register_buffer(\"freqs\", freqs)  # (num_freqs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x: (batch, num_samples, in_features)\n",
    "        Outputs:\n",
    "            out: (batch, num_samples, 2*num_freqs*in_features)\n",
    "        \"\"\"\n",
    "        x_proj = x.unsqueeze(dim=-2) * self.freqs.unsqueeze(dim=-1)  # (num_rays, num_samples, num_freqs, in_features)\n",
    "        x_proj = x_proj.reshape(*x.shape[:-1], -1)  # (num_rays, num_samples, num_freqs*in_features)\n",
    "        out = torch.cat([torch.sin(x_proj), torch.cos(x_proj)],\n",
    "                        dim=-1)  # (num_rays, num_samples, 2*num_freqs*in_features)\n",
    "        return out\n",
    "    \n",
    "class BModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_coords, out_values, dim, pos_encoding=False):\n",
    "        super().__init__()\n",
    "        if pos_encoding:\n",
    "            posenc = PositionalEncoding(8, 20)\n",
    "            d_in = nn.Linear(in_coords * 40, dim)\n",
    "            self.d_in = nn.Sequential(posenc, d_in)\n",
    "        else:\n",
    "            self.d_in = nn.Linear(in_coords, dim)\n",
    "        lin = [nn.Linear(dim, dim) for _ in range(8)]\n",
    "        self.linear_layers = nn.ModuleList(lin)\n",
    "        self.d_out = nn.Linear(dim, out_values)\n",
    "        self.activation = Sine()  # torch.tanh\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.d_in(x))\n",
    "        for l in self.linear_layers:\n",
    "            x = self.activation(l(x))\n",
    "        x = self.d_out(x)\n",
    "        return x\n",
    "\n",
    "class VectorPotentialModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_coords, dim, pos_encoding=False):\n",
    "        super().__init__()\n",
    "        if pos_encoding:\n",
    "            posenc = PositionalEncoding(8, 20)\n",
    "            d_in = nn.Linear(in_coords * 40, dim)\n",
    "            self.d_in = nn.Sequential(posenc, d_in)\n",
    "        else:\n",
    "            self.d_in = nn.Linear(in_coords, dim)\n",
    "        lin = [nn.Linear(dim, dim) for _ in range(8)]\n",
    "        self.linear_layers = nn.ModuleList(lin)\n",
    "        self.d_out = nn.Linear(dim, 3)\n",
    "        self.activation = Sine()  # torch.tanh\n",
    "\n",
    "    def forward(self, x):\n",
    "        coord = x\n",
    "        x = self.activation(self.d_in(x))\n",
    "        for l in self.linear_layers:\n",
    "            x = self.activation(l(x))\n",
    "        a = self.d_out(x)\n",
    "        #\n",
    "        jac_matrix = jacobian(a, coord)\n",
    "        dAy_dx = jac_matrix[:, 1, 0]\n",
    "        dAz_dx = jac_matrix[:, 2, 0]\n",
    "        dAx_dy = jac_matrix[:, 0, 1]\n",
    "        dAz_dy = jac_matrix[:, 2, 1]\n",
    "        dAx_dz = jac_matrix[:, 0, 2]\n",
    "        dAy_dz = jac_matrix[:, 1, 2]\n",
    "        rot_x = dAz_dy - dAy_dz\n",
    "        rot_y = dAx_dz - dAz_dx\n",
    "        rot_z = dAy_dx - dAx_dy\n",
    "        b = torch.stack([rot_x, rot_y, rot_z], -1)\n",
    "        #\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PotentialModel(nn.Module):\n",
    "\n",
    "    def __init__(self, b_n, r_p):\n",
    "        super().__init__()\n",
    "        self.register_buffer('b_n', b_n)\n",
    "        self.register_buffer('r_p', r_p)\n",
    "        c = np.zeros((1, 3))\n",
    "        c[:, 2] = (1 / np.sqrt(2 * np.pi))\n",
    "        c = torch.tensor(c, dtype=torch.float32, )\n",
    "        self.register_buffer('c', c)\n",
    "\n",
    "    def forward(self, coord):\n",
    "        v1 = self.b_n[:, None]\n",
    "        v2 = 2 * np.pi * ((-self.r_p[:, None] + coord[None, :] + self.c[None]) ** 2).sum(-1) ** 0.5\n",
    "        potential = torch.sum(v1 / v2, dim=0)\n",
    "        return potential\n",
    "    \n",
    "def get_potential_boundary(b_n, height, batch_size=2048):\n",
    "    assert not np.any(np.isnan(b_n)), 'Invalid data value'\n",
    "\n",
    "    cube_shape = (*b_n.shape, height)\n",
    "\n",
    "    b_n = b_n.reshape((-1)).astype(np.float32)\n",
    "    coords = [np.stack(np.mgrid[:cube_shape[0], :cube_shape[1], cube_shape[2] - 2:cube_shape[2] + 1], -1),\n",
    "              np.stack(np.mgrid[:cube_shape[0], -1:2, :cube_shape[2]], -1),\n",
    "              np.stack(np.mgrid[:cube_shape[0], cube_shape[1] - 2:cube_shape[1] + 1, :cube_shape[2]], -1),\n",
    "              np.stack(np.mgrid[-1:2, :cube_shape[1], :cube_shape[2]], -1),\n",
    "              np.stack(np.mgrid[cube_shape[0] - 2:cube_shape[0] + 1, :cube_shape[1], :cube_shape[2]], -1), ]\n",
    "    coords_shape = [c.shape[:-1] for c in coords]\n",
    "    flat_coords = np.concatenate([c.reshape(((-1, 3))) for c in coords])\n",
    "\n",
    "    r_p = np.stack(np.mgrid[:cube_shape[0], :cube_shape[1], :1], -1).reshape((-1, 3))\n",
    "\n",
    "    # torch code\n",
    "    # r = (x * y, 3); coords = (x*y*z, 3), c = (1, 3)\n",
    "    # --> (x * y, x * y * z, 3) --> (x * y, x * y * z) --> (x * y * z)\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    with torch.no_grad():\n",
    "        b_n = torch.tensor(b_n, dtype=torch.float32, )\n",
    "        r_p = torch.tensor(r_p, dtype=torch.float32, )\n",
    "        model = nn.DataParallel(PotentialModel(b_n, r_p, )).to(device)\n",
    "\n",
    "        flat_coords = torch.tensor(flat_coords, dtype=torch.float32, )\n",
    "\n",
    "        potential = []\n",
    "        for coord, in tqdm(DataLoader(TensorDataset(flat_coords), batch_size=batch_size, num_workers=2),\n",
    "                           desc='Potential Boundary'):\n",
    "            coord = coord.to(device)\n",
    "            p_batch = model(coord)\n",
    "            potential += [p_batch.cpu()]\n",
    "\n",
    "    potential = torch.cat(potential).numpy()\n",
    "    idx = 0\n",
    "    fields = []\n",
    "    for s in coords_shape:\n",
    "        p = potential[idx:idx + np.prod(s)].reshape(s)\n",
    "        b = - 1 * np.stack(np.gradient(p, axis=[0, 1, 2], edge_order=2), axis=-1)\n",
    "        fields += [b]\n",
    "        idx += np.prod(s)\n",
    "\n",
    "    fields = [fields[0][:, :, 1].reshape((-1, 3)),\n",
    "              fields[1][:, 1, :].reshape((-1, 3)), fields[2][:, 1, :].reshape((-1, 3)),\n",
    "              fields[3][1, :, :].reshape((-1, 3)), fields[4][1, :, :].reshape((-1, 3))]\n",
    "    coords = [coords[0][:, :, 1].reshape((-1, 3)),\n",
    "              coords[1][:, 1, :].reshape((-1, 3)), coords[2][:, 1, :].reshape((-1, 3)),\n",
    "              coords[3][1, :, :].reshape((-1, 3)), coords[4][1, :, :].reshape((-1, 3))]\n",
    "    return np.concatenate(coords), np.concatenate(fields)\n",
    "\n",
    "def _load_potential_field_data(hmi_cube, height, reduce):\n",
    "    if reduce > 1:\n",
    "        hmi_cube = block_reduce(hmi_cube, (reduce, reduce, 1), func=np.mean)\n",
    "        height = height // reduce\n",
    "    pf_batch_size = int(1024 * 512 ** 2 / np.prod(hmi_cube.shape[:2]))  # adjust batch to AR size\n",
    "    pf_coords, pf_values = get_potential_boundary(hmi_cube[:, :, 2], height, batch_size=pf_batch_size)\n",
    "    pf_values = np.array(pf_values, dtype=np.float32)\n",
    "    pf_coords = np.array(pf_coords, dtype=np.float32) * reduce # expand to original coordinate spacing\n",
    "    pf_err = np.zeros_like(pf_values)\n",
    "    return pf_coords, pf_err, pf_values\n",
    "\n",
    "def _plot_data(n_hmi_cube, plot_path, b_norm):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    axs[0].imshow(n_hmi_cube[..., 0].transpose(), vmin=-b_norm, vmax=b_norm, cmap='gray', origin='lower')\n",
    "    axs[1].imshow(n_hmi_cube[..., 1].transpose(), vmin=-b_norm, vmax=b_norm, cmap='gray', origin='lower')\n",
    "    axs[2].imshow(n_hmi_cube[..., 2].transpose(), vmin=-b_norm, vmax=b_norm, cmap='gray', origin='lower')\n",
    "    plt.savefig(os.path.join(plot_path, 'b.jpg'))\n",
    "    plt.close()\n",
    "\n",
    "def prep_b_data(b_bottom,\n",
    "                height, spatial_norm, b_norm,\n",
    "                potential_boundary=True, potential_strides=4,\n",
    "                plot=False, plot_path=None):\n",
    "    # load coordinates\n",
    "    mf_coords = np.stack(np.mgrid[:b_bottom.shape[0], :b_bottom.shape[1], :1], -1)\n",
    "    # flatten data\n",
    "    mf_coords = mf_coords.reshape((-1, 3))\n",
    "    mf_values = b_bottom.reshape((-1, 3))\n",
    "    # load potential field\n",
    "    if potential_boundary:\n",
    "        pf_coords, pf_err, pf_values = _load_potential_field_data(b_bottom, height, potential_strides)\n",
    "        # concatenate pf data points\n",
    "        coords = np.concatenate([pf_coords, mf_coords])\n",
    "        values = np.concatenate([pf_values, mf_values])\n",
    "    else:\n",
    "        coords = mf_coords\n",
    "        values = mf_values\n",
    "\n",
    "    coords = coords.astype(np.float32)\n",
    "    values = values.astype(np.float32)\n",
    "\n",
    "    # normalize data\n",
    "    values = Normalize(-b_norm, b_norm, clip=False)(values) * 2 - 1\n",
    "\n",
    "    # apply spatial normalization\n",
    "    coords = coords / spatial_norm\n",
    "\n",
    "    # stack to numpy array\n",
    "    data = np.stack([coords, values], 1)\n",
    "\n",
    "    if plot:\n",
    "        _plot_data(b_bottom, plot_path, b_norm)\n",
    "\n",
    "    return data\n",
    "\n",
    "def calculate_loss(b, coords):\n",
    "    jac_matrix = jacobian(b, coords)\n",
    "    dBx_dx = jac_matrix[:, 0, 0]\n",
    "    dBy_dx = jac_matrix[:, 1, 0]\n",
    "    dBz_dx = jac_matrix[:, 2, 0]\n",
    "    dBx_dy = jac_matrix[:, 0, 1]\n",
    "    dBy_dy = jac_matrix[:, 1, 1]\n",
    "    dBz_dy = jac_matrix[:, 2, 1]\n",
    "    dBx_dz = jac_matrix[:, 0, 2]\n",
    "    dBy_dz = jac_matrix[:, 1, 2]\n",
    "    dBz_dz = jac_matrix[:, 2, 2]\n",
    "    #\n",
    "    rot_x = dBz_dy - dBy_dz\n",
    "    rot_y = dBx_dz - dBz_dx\n",
    "    rot_z = dBy_dx - dBx_dy\n",
    "    #\n",
    "    j = torch.stack([rot_x, rot_y, rot_z], -1)\n",
    "    jxb = torch.cross(j, b, -1)\n",
    "    force_loss = torch.sum(jxb ** 2, dim=-1) / (torch.sum(b ** 2, dim=-1) + 1e-7)\n",
    "    divergence_loss = (dBx_dx + dBy_dy + dBz_dz) ** 2\n",
    "    return divergence_loss, force_loss\n",
    "\n",
    "class BoundaryDataset(Dataset):\n",
    "\n",
    "    def __init__(self, batches_path):\n",
    "        self.batches_path = batches_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return np.load(self.batches_path, mmap_mode='r').shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # lazy load data\n",
    "        d = np.load(self.batches_path, mmap_mode='r')[idx]\n",
    "        d = np.copy(d)\n",
    "        coord, field = d[:, 0],  d[:, 1]\n",
    "        return coord, field\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, cube_shape, norm, z=0):\n",
    "        coordinates = np.stack(np.mgrid[:cube_shape[0],\n",
    "                               :cube_shape[1]], -1)\n",
    "        self.coordinates = coordinates\n",
    "        self.coordinates_flat = coordinates.reshape((-1, 2))\n",
    "        self.norm = norm\n",
    "        self.z = z / self.norm\n",
    "\n",
    "    def __len__(self, ):\n",
    "        return self.coordinates_flat.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        coord = self.coordinates_flat[idx]\n",
    "        scaled_coord = [coord[0] / self.norm,\n",
    "                        coord[1] / self.norm,\n",
    "                        self.z]\n",
    "        return np.array(scaled_coord, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coords(xbounds, ybounds, zbounds):\n",
    "    return np.stack(np.mgrid[xbounds[0]:xbounds[1]+1, ybounds[0]:ybounds[1]+1, zbounds[0]:zbounds[1]+1], axis=-1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NF2Trainer:\n",
    "\n",
    "    def __init__(self, base_path, b_bottom, height, spatial_norm, b_norm, meta_info=None, dim=256,\n",
    "                 positional_encoding=False, meta_path=None, use_potential_boundary=True, potential_strides=1, use_vector_potential=False,\n",
    "                 w_div=0.1, w_ff=0.1, decay_iterations=None,\n",
    "                 device=None, work_directory=None):\n",
    "        \"\"\"Magnetic field extrapolations trainer\n",
    "\n",
    "        :param base_path: path to the results folder.\n",
    "        :param b_bottom: magnetic field data (x, y, (Bp, -Bt, Br)).\n",
    "        :param height: height of simulation volume.\n",
    "        :param spatial_norm: normalization of coordinate axis.\n",
    "        :param b_norm: normalization of magnetic field strength.\n",
    "        :param meta_info: additional data information. stored in the save state.\n",
    "        :param dim: number of neurons per layer (8 layers).\n",
    "        :param positional_encoding: use positional encoding.\n",
    "        :param meta_path: start from a pre-learned simulation state.\n",
    "        :param use_potential_boundary: use potential field as boundary condition. If None use an open boundary.\n",
    "        :param potential_strides: use binned potential field boundary condition. Only applies if use_potential_boundary = True.\n",
    "        :param use_vector_potential: derive the magnetic field from a vector potential.\n",
    "        :param w_div: weighting parameter for divergence freeness of the simulation.\n",
    "        :param w_ff: weighting parameter for force freeness of the simulation.\n",
    "        :param decay_iterations: decay weighting for boundary condition (w_bc=1000) over n iterations to 1.\n",
    "        :param device: device for model training.\n",
    "        :param work_directory: directory to store scratch data (prepared batches).\n",
    "        \"\"\"\n",
    "\n",
    "        # general parameters\n",
    "        self.base_path = base_path\n",
    "        work_directory = base_path if work_directory is None else work_directory\n",
    "        self.work_directory = work_directory\n",
    "        self.save_path = os.path.join(base_path, 'extrapolation_result.nf2')\n",
    "        self.checkpoint_path = os.path.join(base_path, 'checkpoint.pt')\n",
    "\n",
    "        # data parameters\n",
    "        self.spatial_norm = spatial_norm\n",
    "        self.height = height\n",
    "        self.b_norm = b_norm\n",
    "        self.meta_info = meta_info\n",
    "\n",
    "        # init directories\n",
    "        os.makedirs(base_path, exist_ok=True)\n",
    "        os.makedirs(work_directory, exist_ok=True)\n",
    "\n",
    "        # init log\n",
    "        self.log = logging.getLogger()\n",
    "        self.log.setLevel(logging.INFO)\n",
    "        for hdlr in self.log.handlers[:]:  # remove all old handlers\n",
    "            self.log.removeHandler(hdlr)\n",
    "        self.log.addHandler(logging.FileHandler(\"{0}/{1}.log\".format(base_path, \"info_log\")))  # set the new file handler\n",
    "        self.log.addHandler(logging.StreamHandler())  # set the new console handler\n",
    "\n",
    "        # log settings\n",
    "        self.log.info('Configuration:')\n",
    "        self.log.info(\n",
    "            'dim: %d, w_div: %f, w_ff: %f, decay_iterations: %s, potential: %s, vector_potential: %s, ' % (\n",
    "                dim, w_div, w_ff, str(decay_iterations), str(use_potential_boundary),\n",
    "                str(use_vector_potential)))\n",
    "\n",
    "        # setup device\n",
    "        n_gpus = torch.cuda.device_count()\n",
    "        device_names = [get_device_name(i) for i in range(n_gpus)]\n",
    "        if device is None:\n",
    "            device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.log.info('Using device: %s (gpus %d) %s' % (str(device), n_gpus, str(device_names)))\n",
    "        self.device = device\n",
    "\n",
    "        # prepare data\n",
    "        self.b_bottom = b_bottom\n",
    "\n",
    "        # load dataset\n",
    "        self.data = prep_b_data(b_bottom, height, spatial_norm, b_norm,\n",
    "                                plot=True, plot_path=base_path,\n",
    "                                potential_boundary=use_potential_boundary, potential_strides=potential_strides)\n",
    "        # self.cube_shape = [*b_bottom.shape[:-1], height]\n",
    "\n",
    "        Nx, Ny, _ = b_bottom.shape\n",
    "        Nz = height\n",
    "\n",
    "        self.cube_shape = (Nx, Ny, Nz)\n",
    "\n",
    "        # init model\n",
    "        if use_vector_potential:\n",
    "            model = VectorPotentialModel(3, dim, pos_encoding=positional_encoding)\n",
    "        else:\n",
    "            model = BModel(3, 3, dim, pos_encoding=positional_encoding)\n",
    "        parallel_model = nn.DataParallel(model)\n",
    "        parallel_model.to(device)\n",
    "        opt = torch.optim.Adam(parallel_model.parameters(), lr=5e-4)\n",
    "        self.model = model\n",
    "        self.parallel_model = parallel_model\n",
    "\n",
    "        # load last state\n",
    "        if os.path.exists(self.checkpoint_path):\n",
    "            state_dict = torch.load(self.checkpoint_path, map_location=device)\n",
    "            start_iteration = state_dict['iteration']\n",
    "            model.load_state_dict(state_dict['m'])\n",
    "            opt.load_state_dict(state_dict['o'])\n",
    "            history = state_dict['history']\n",
    "            w_bc = state_dict['w_bc']\n",
    "            self.log.info('Resuming training from iteration %d' % start_iteration)\n",
    "        else:\n",
    "            if meta_path:\n",
    "                state_dict = torch.load(meta_path, map_location=device)['model'].state_dict() \\\n",
    "                    if meta_path.endswith('nf2') else torch.load(meta_path, map_location=device)['m']\n",
    "                model.load_state_dict(state_dict)\n",
    "                opt = torch.optim.Adam(parallel_model.parameters(), lr=5e-5)\n",
    "                self.log.info('Loaded meta state: %s' % meta_path)\n",
    "            # init\n",
    "            start_iteration = 0\n",
    "            w_bc = 1000 if decay_iterations else 1\n",
    "            history = {'iteration': [], 'height': [],\n",
    "                       'b_loss': [], 'divergence_loss': [], 'force_loss': [], 'sigma_angle': []}\n",
    "\n",
    "        self.opt = opt\n",
    "        self.start_iteration = start_iteration\n",
    "        self.history = history\n",
    "        self.w_bc = w_bc\n",
    "        self.w_bc_decay = (1 / 1000) ** (1 / decay_iterations) if decay_iterations is not None else 1\n",
    "        self.w_div, self.w_ff = w_div, w_ff\n",
    "\n",
    "        collocation_coords = coords((0, Nx-1), (0, Ny-1), (0, Nz-1)).reshape(-1, 3)\n",
    "        normalized_collocation_coords = collocation_coords / self.spatial_norm\n",
    "        self.normalized_collocation_coords = torch.tensor(normalized_collocation_coords)\n",
    "\n",
    "    def train(self, total_iterations, batch_size, log_interval=100, validation_interval=100, num_workers=None):\n",
    "        \"\"\"Start magnetic field extrapolation fit.\n",
    "\n",
    "        :param total_iterations: number of iterations for training.\n",
    "        :param batch_size: number of samples per iteration.\n",
    "        :param log_interval: log training details every nth iteration.\n",
    "        :param validation_interval: evaluate simulation every nth iteration.\n",
    "        :param num_workers: number of workers for data loading (default system spec).\n",
    "        :return: path of the final save state.\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        num_workers = os.cpu_count() if num_workers is None else num_workers\n",
    "\n",
    "        model = self.parallel_model\n",
    "        opt = self.opt\n",
    "        device = self.device\n",
    "        w_div, w_ff = self.w_div, self.w_ff\n",
    "\n",
    "        # init\n",
    "        scheduler = ExponentialLR(opt, gamma=(5e-5 / 5e-4) ** (1 / total_iterations))\n",
    "        iterations = total_iterations - self.start_iteration\n",
    "        if iterations <= 0:\n",
    "            self.log.info('Training already finished!')\n",
    "            return self.save_path\n",
    "\n",
    "        # init loader\n",
    "        data_loader, batches_path = self._init_loader(batch_size, self.data, num_workers, iterations)\n",
    "        \n",
    "        model.train()\n",
    "        for iter, (boundary_coords, boundary_b) in tqdm(enumerate(data_loader, start=self.start_iteration),\n",
    "                                                           total=len(data_loader), desc='Training'):\n",
    "\n",
    "            boundary_coords, boundary_b= boundary_coords.to(device), boundary_b.to(device)\n",
    "\n",
    "            perm = torch.randperm(self.normalized_collocation_coords.shape[0])\n",
    "            idx = perm[:batch_size]\n",
    "            co_coords = self.normalized_collocation_coords[idx].to(device)\n",
    "\n",
    "            # concatenate boundary and random points\n",
    "            # n_boundary_coords = boundary_coords.shape[0]\n",
    "            # r = torch.cat([boundary_coords, co_coords], 0)\n",
    "            r = co_coords\n",
    "            r.requires_grad = True\n",
    "\n",
    "            # forward step\n",
    "            B = model(r)\n",
    "\n",
    "            # if iter == 0:\n",
    "            #     model.eval()\n",
    "            #     torch.save({'model': self.model,\n",
    "            #         'cube_shape': self.cube_shape,\n",
    "            #         'b_norm': self.b_norm,\n",
    "            #         'spatial_norm': self.spatial_norm,\n",
    "            #         'meta_info': self.meta_info}, os.path.join(self.base_path, 'fields_%06d.nf2' % iter))\n",
    "            #     self.plot_sample(iter-1, batch_size=batch_size)\n",
    "            #     model.train()\n",
    "\n",
    "            # compute boundary loss\n",
    "            # boundary_B = B[:n_boundary_coords]\n",
    "            boundary_B = model(boundary_coords)\n",
    "            # bc_loss = torch.abs(boundary_B - boundary_b)\n",
    "            # bc_loss = torch.mean(bc_loss.pow(2).sum(-1))\n",
    "\n",
    "            bc_loss = torch.sum((boundary_B - boundary_b)**2, dim=-1)\n",
    "            bc_loss = torch.mean(bc_loss)\n",
    "            # compute div and ff loss\n",
    "            # divergence_loss, force_free_loss = calculate_loss(b, coords)\n",
    "\n",
    "            dBx_dr = torch.autograd.grad(B[:, 0], r, torch.ones_like(B[:, 0]), retain_graph=True, create_graph=True)[0]\n",
    "            dBy_dr = torch.autograd.grad(B[:, 1], r, torch.ones_like(B[:, 1]), retain_graph=True, create_graph=True)[0]\n",
    "            dBz_dr = torch.autograd.grad(B[:, 2], r, torch.ones_like(B[:, 2]), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "            dBx_dx = dBx_dr[:, 0]\n",
    "            dBx_dy = dBx_dr[:, 1]\n",
    "            dBx_dz = dBx_dr[:, 2]\n",
    "\n",
    "            dBy_dx = dBy_dr[:, 0]\n",
    "            dBy_dy = dBy_dr[:, 1]\n",
    "            dBy_dz = dBy_dr[:, 2]\n",
    "\n",
    "            dBz_dx = dBz_dr[:, 0]\n",
    "            dBz_dy = dBz_dr[:, 1]\n",
    "            dBz_dz = dBz_dr[:, 2]\n",
    "\n",
    "            rot_x = dBz_dy - dBy_dz\n",
    "            rot_y = dBx_dz - dBz_dx\n",
    "            rot_z = dBy_dx - dBx_dy\n",
    "\n",
    "            J = torch.stack([rot_x, rot_y, rot_z], -1)\n",
    "            JxB = torch.cross(J, B, dim=-1)\n",
    "\n",
    "            divB = dBx_dx + dBy_dy + dBz_dz\n",
    "\n",
    "            force_free_loss = torch.sum(JxB**2, dim=-1) / (torch.sum(B**2, dim=-1) + 1e-7)\n",
    "            force_free_loss = torch.mean(force_free_loss)\n",
    "            divergence_loss = torch.sum((divB)**2, dim=-1)\n",
    "            divergence_loss = torch.mean(divergence_loss)\n",
    "\n",
    "            loss = self.w_bc*bc_loss + w_ff*force_free_loss + w_div*divergence_loss\n",
    "\n",
    "            if iter == 0:\n",
    "                model.eval()\n",
    "                torch.save({'BC_loss': bc_loss.detach().cpu().numpy(),\n",
    "                    'w_bc': self.w_bc,\n",
    "                    'divergence_loss': divergence_loss.mean().detach().cpu().numpy(),\n",
    "                    'w_div': w_div,\n",
    "                    'force_loss': force_free_loss.mean().detach().cpu().numpy(),\n",
    "                    'w_ff': w_ff,}, os.path.join(self.base_path, 'loss_%06d.nf2' % iter))\n",
    "                torch.save({'model': self.model,\n",
    "                    'cube_shape': self.cube_shape,\n",
    "                    'b_norm': self.b_norm,\n",
    "                    'spatial_norm': self.spatial_norm,\n",
    "                    'meta_info': self.meta_info}, os.path.join(self.base_path, 'fields_%06d.nf2' % iter))\n",
    "                model.train()\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            opt.step()\n",
    "\n",
    "            if (log_interval > 0 and (iter + 1) % log_interval == 0):\n",
    "                # log loss\n",
    "                self.log.info('[Iteration %06d/%06d] [loss: %.08f] [bc_loss: %.08f; div_loss: %.08f; ff_loss: %.08f] [w_bc: %f, LR: %f] [%s]' %\n",
    "                        (iter + 1, total_iterations,\n",
    "                        loss,\n",
    "                        self.w_bc*bc_loss,\n",
    "                        w_ff*force_free_loss,\n",
    "                        w_div*divergence_loss,\n",
    "                        self.w_bc,\n",
    "                        scheduler.get_last_lr()[0],\n",
    "                        datetime.now() - start_time))\n",
    "                \n",
    "                torch.save({'BC_loss': bc_loss.detach().cpu().numpy(),\n",
    "                            'lambda_BC': self.w_bc,\n",
    "                            'divergence_loss': divergence_loss.detach().cpu().numpy(),\n",
    "                            'lambda_div': w_div,\n",
    "                            'force_loss': force_free_loss.detach().cpu().numpy(),\n",
    "                            'lambda_ff': w_ff,\n",
    "                            'LR':scheduler.get_last_lr()[0]}, \n",
    "                            os.path.join(base_path, 'loss_%06d.nf2' % iter))\n",
    "                torch.save({'model': model,\n",
    "                            'cube_shape': self.cube_shape,\n",
    "                            'b_norm': b_norm,\n",
    "                            'spatial_norm': spatial_norm,\n",
    "                            'meta_info': meta_info}, \n",
    "                            os.path.join(base_path, 'fields_%06d.nf2' % iter))\n",
    "\n",
    "            # update training parameters\n",
    "            if self.w_bc > 1:\n",
    "                self.w_bc *= self.w_bc_decay\n",
    "            if scheduler.get_last_lr()[0] > 5e-5:\n",
    "                scheduler.step()\n",
    "\n",
    "        # save final model state\n",
    "        torch.save({'BC_loss': bc_loss.detach().cpu().numpy(),\n",
    "                    'w_bc': self.w_bc,\n",
    "                    'divergence_loss': divergence_loss.detach().cpu().numpy(),\n",
    "                    'w_div': w_div,\n",
    "                    'force_loss': force_free_loss.detach().cpu().numpy(),\n",
    "                    'w_ff': w_ff,\n",
    "                    'LR':scheduler.get_last_lr()[0]}, \n",
    "                    os.path.join(base_path, 'loss_final.nf2'))\n",
    "        torch.save({'model': model,\n",
    "                    'cube_shape': self.cube_shape,\n",
    "                    'b_norm': b_norm,\n",
    "                    'spatial_norm': spatial_norm,\n",
    "                    'meta_info': meta_info}, \n",
    "                    os.path.join(base_path, 'fields_final.nf2'))\n",
    "        torch.save({'m': model.state_dict(),\n",
    "                    'o': opt.state_dict(), },\n",
    "                    os.path.join(base_path, 'model_final.pt'))\n",
    "        # cleanup\n",
    "        os.remove(batches_path)\n",
    "\n",
    "        return self.save_path\n",
    "    \n",
    "\n",
    "    def _init_loader(self, batch_size, data, num_workers, iterations):\n",
    "        # shuffle data\n",
    "        r = np.random.permutation(data.shape[0])\n",
    "        data = data[r]\n",
    "        # adjust to batch size\n",
    "        pad = batch_size - data.shape[0] % batch_size\n",
    "        data = np.concatenate([data, data[:pad]])\n",
    "        # split data into batches\n",
    "        n_batches = data.shape[0] // batch_size\n",
    "        batches = np.array(np.split(data, n_batches), dtype=np.float32)\n",
    "        # store batches to disk\n",
    "        batches_path = os.path.join(self.work_directory, 'batches.npy')\n",
    "        np.save(batches_path, batches)\n",
    "        # create data loaders\n",
    "        dataset = BoundaryDataset(batches_path)\n",
    "        # create loader\n",
    "        data_loader = DataLoader(dataset, batch_size=None, num_workers=num_workers, pin_memory=True,\n",
    "                                 sampler=RandomSampler(dataset, replacement=True, num_samples=iterations))\n",
    "        return data_loader, batches_path\n",
    "\n",
    "    def save(self, iteration):\n",
    "        torch.save({'model': self.model,\n",
    "                    'cube_shape': self.cube_shape,\n",
    "                    'b_norm': self.b_norm,\n",
    "                    'spatial_norm': self.spatial_norm,\n",
    "                    'meta_info': self.meta_info}, self.save_path)\n",
    "        torch.save({'iteration': iteration + 1,\n",
    "                    'm': self.model.state_dict(),\n",
    "                    'o': self.opt.state_dict(),\n",
    "                    'history': self.history,\n",
    "                    'lambda_B': self.lambda_B},\n",
    "                   self.checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "dim: 256, w_div: 0.100000, w_ff: 0.100000, decay_iterations: 25000, potential: True, vector_potential: False, \n",
      "Using device: cuda (gpus 1) ['NVIDIA GeForce RTX 3060']\n",
      "Potential Boundary: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "Training:   0%|          | 98/50000 [00:06<52:05, 15.97it/s] [Iteration 000100/050000] [loss: 31.24823189] [bc_loss: 31.16707230; div_loss: 0.00014445; ff_loss: 0.08101460] [w_bc: 973.016041, LR: 0.000498] [0:00:06.343937]\n",
      "Training:   0%|          | 198/50000 [00:12<52:11, 15.90it/s][Iteration 000200/050000] [loss: 29.34061050] [bc_loss: 29.28204727; div_loss: 0.00061701; ff_loss: 0.05794651] [w_bc: 946.498652, LR: 0.000495] [0:00:12.690126]\n",
      "Training:   0%|          | 200/50000 [00:12<52:32, 15.80it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m NF2Trainer(base_path, b_bottom, height, spatial_norm, b_norm, \n\u001b[1;32m      2\u001b[0m                      meta_info\u001b[39m=\u001b[39mmeta_info, dim\u001b[39m=\u001b[39mdim, positional_encoding\u001b[39m=\u001b[39mpositional_encoding, \n\u001b[1;32m      3\u001b[0m                      meta_path\u001b[39m=\u001b[39mmeta_path, use_potential_boundary\u001b[39m=\u001b[39muse_potential_boundary, \n\u001b[1;32m      4\u001b[0m                      potential_strides\u001b[39m=\u001b[39mpotential_strides, use_vector_potential\u001b[39m=\u001b[39muse_vector_potential,\n\u001b[1;32m      5\u001b[0m                      w_div\u001b[39m=\u001b[39mw_div, w_ff\u001b[39m=\u001b[39mw_ff, decay_iterations\u001b[39m=\u001b[39mdecay_iterations,\n\u001b[1;32m      6\u001b[0m                      device\u001b[39m=\u001b[39mdevice, work_directory\u001b[39m=\u001b[39mwork_directory)\n\u001b[0;32m----> 8\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(total_iterations, batch_size, \n\u001b[1;32m      9\u001b[0m               log_interval\u001b[39m=\u001b[39;49mlog_interval, validation_interval\u001b[39m=\u001b[39;49mvalidation_interval, \n\u001b[1;32m     10\u001b[0m               num_workers\u001b[39m=\u001b[39;49mnum_workers)\n",
      "Cell \u001b[0;32mIn[7], line 160\u001b[0m, in \u001b[0;36mNF2Trainer.train\u001b[0;34m(self, total_iterations, batch_size, log_interval, validation_interval, num_workers)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39miter\u001b[39m, (boundary_coords, boundary_b) \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(data_loader, start\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_iteration),\n\u001b[1;32m    156\u001b[0m                                                    total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(data_loader), desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    158\u001b[0m     boundary_coords, boundary_b\u001b[39m=\u001b[39m boundary_coords\u001b[39m.\u001b[39mto(device), boundary_b\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> 160\u001b[0m     perm \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrandperm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized_collocation_coords\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m    161\u001b[0m     idx \u001b[39m=\u001b[39m perm[:batch_size]\n\u001b[1;32m    162\u001b[0m     co_coords \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalized_collocation_coords[idx]\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = NF2Trainer(base_path, b_bottom, height, spatial_norm, b_norm, \n",
    "                     meta_info=meta_info, dim=dim, positional_encoding=positional_encoding, \n",
    "                     meta_path=meta_path, use_potential_boundary=use_potential_boundary, \n",
    "                     potential_strides=potential_strides, use_vector_potential=use_vector_potential,\n",
    "                     w_div=w_div, w_ff=w_ff, decay_iterations=decay_iterations,\n",
    "                     device=device, work_directory=work_directory)\n",
    "\n",
    "trainer.train(total_iterations, batch_size, \n",
    "              log_interval=log_interval, validation_interval=validation_interval, \n",
    "              num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
