{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from pinf.analytical_field import get_analytic_b_field\n",
    "from pinf.performance_metrics import metrics\n",
    "from pinf.trainer import NF2Trainer\n",
    "from pinf.unpack import load_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = get_analytic_b_field(n=1, m=1, l=0.3, psi=np.pi/2, resolution=64, bounds=[-1, 1, -1, 1, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "dim: 256, lambda_div: 0.100000, lambda_ff: 0.100000, decay_iterations: 25000, potential: True, vector_potential: False, \n",
      "Using device: cuda (gpus 1) ['NVIDIA GeForce RTX 3060']\n",
      "Potential Boundary: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\"\n",
    "\n",
    "base_path = './run2'\n",
    "meta_path = None\n",
    "\n",
    "bin = 1\n",
    "\n",
    "height = 64\n",
    "spatial_norm = 32\n",
    "b_norm = 100\n",
    "\n",
    "meta_info = None\n",
    "dim = 256\n",
    "positional_encoding = False\n",
    "use_potential_boundary = True\n",
    "potential_strides = 1\n",
    "use_vector_potential = False\n",
    "lambda_div = 1e-1\n",
    "lambda_ff = 1e-1\n",
    "decay_iterations = 25000\n",
    "device = None\n",
    "work_directory = None\n",
    "\n",
    "total_iterations = 50000\n",
    "batch_size = 10000\n",
    "log_interval = 100\n",
    "validation_interval = 100\n",
    "num_workers = 4\n",
    "\n",
    "# init logging\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "log = logging.getLogger()\n",
    "log.setLevel(logging.INFO)\n",
    "for hdlr in log.handlers[:]:  # remove all old handlers\n",
    "    log.removeHandler(hdlr)\n",
    "log.addHandler(logging.FileHandler(\"{0}/{1}.log\".format(base_path, \"info_log\")))  # set the new file handler\n",
    "log.addHandler(logging.StreamHandler())  # set the new console handler\n",
    "\n",
    "base_path = os.path.join(base_path, 'dim%d_bin%d_pf%s_ld%s_lf%s' % (\n",
    "        dim, bin, str(use_potential_boundary), lambda_div, lambda_ff))\n",
    "\n",
    "b_cube = b[:, :, 0, :]\n",
    "\n",
    "trainer = NF2Trainer(base_path, b_cube, height, spatial_norm, b_norm, \n",
    "                     meta_info=meta_info, dim=dim, positional_encoding=positional_encoding, \n",
    "                     meta_path=meta_path, use_potential_boundary=use_potential_boundary, \n",
    "                     potential_strides=potential_strides, use_vector_potential=use_vector_potential,\n",
    "                     lambda_div=lambda_div, lambda_ff=lambda_ff, decay_iterations=decay_iterations,\n",
    "                     device=device, work_directory=work_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/50000 [00:00<?, ?it/s][Iteration 000001/050000] [BC: 0.00003453; Div: 0.00005190; For: 0.00011409] [0:00:03.710738]\n",
      "Lambda B: 0.999724\n",
      "LR: 0.000058\n",
      "Validation: 100%|██████████| 27/27 [00:00<00:00, 65.39it/s]\n",
      "Validation [Cube: BC: 0.777; Div: 0.007; For: 0.022; Sig: 3.882]\n",
      "Training:   0%|          | 37/50000 [00:09<3:34:20,  3.89it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m start_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[0;32m----> 2\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(total_iterations, batch_size, \n\u001b[1;32m      3\u001b[0m               log_interval\u001b[39m=\u001b[39;49mlog_interval, validation_interval\u001b[39m=\u001b[39;49mvalidation_interval, \n\u001b[1;32m      4\u001b[0m               num_workers\u001b[39m=\u001b[39;49mnum_workers)\n\u001b[1;32m      5\u001b[0m log\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTOTAL RUNTIME: \u001b[39m\u001b[39m{\u001b[39;00mdatetime\u001b[39m.\u001b[39mnow()\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mstart_time\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/pinn_study/organize/pinf/trainer.py:212\u001b[0m, in \u001b[0;36mNF2Trainer.train\u001b[0;34m(self, total_iterations, batch_size, log_interval, validation_interval, num_workers)\u001b[0m\n\u001b[1;32m    208\u001b[0m (b_diff \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlambda_B \u001b[39m+\u001b[39m\n\u001b[1;32m    209\u001b[0m  divergence_loss\u001b[39m.\u001b[39mmean() \u001b[39m*\u001b[39m lambda_div \u001b[39m+\u001b[39m\n\u001b[1;32m    210\u001b[0m  force_loss\u001b[39m.\u001b[39mmean() \u001b[39m*\u001b[39m lambda_ff)\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    211\u001b[0m \u001b[39m# update step\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mclip_grad_norm_(model\u001b[39m.\u001b[39;49mparameters(), \u001b[39m0.1\u001b[39;49m)\n\u001b[1;32m    213\u001b[0m opt\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    215\u001b[0m \u001b[39m# save loss information\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:76\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mfor\u001b[39;00m ((device, _), [grads]) \u001b[39min\u001b[39;00m grouped_grads\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m (foreach \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m foreach) \u001b[39mand\u001b[39;00m _has_foreach_support(grads, device\u001b[39m=\u001b[39mdevice):\n\u001b[0;32m---> 76\u001b[0m         torch\u001b[39m.\u001b[39;49m_foreach_mul_(grads, clip_coef_clamped\u001b[39m.\u001b[39;49mto(device))  \u001b[39m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[39melif\u001b[39;00m foreach:\n\u001b[1;32m     78\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mforeach=True was passed, but can\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mt use the foreach API on \u001b[39m\u001b[39m{\u001b[39;00mdevice\u001b[39m.\u001b[39mtype\u001b[39m}\u001b[39;00m\u001b[39m tensors\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "trainer.train(total_iterations, batch_size, \n",
    "              log_interval=log_interval, validation_interval=validation_interval, \n",
    "              num_workers=num_workers)\n",
    "log.info(f'TOTAL RUNTIME: {datetime.now() - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
