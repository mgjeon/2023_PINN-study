{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'b.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m b \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mb.npy\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m height \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[1;32m      9\u001b[0m spatial_norm \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'b.npy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\"\n",
    "\n",
    "import numpy as np\n",
    "b = np.load(\"b.npy\")\n",
    "\n",
    "height = 64\n",
    "spatial_norm = 32\n",
    "b_norm = 100\n",
    "\n",
    "meta_info = None\n",
    "log_interval = 100\n",
    "\n",
    "total_iterations = 50000\n",
    "\n",
    "decay_iterations = 25000\n",
    "w_bc = 1000\n",
    "w_bc_decay = (1 / 1000) ** (1 / decay_iterations)\n",
    "w_div = 0.1\n",
    "w_ff = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_bottom = b[:, :, 0, :]\n",
    "\n",
    "Nx, Ny, _ = b_bottom.shape\n",
    "Nz = height\n",
    "\n",
    "cube_shape = (Nx, Ny, Nz)\n",
    "\n",
    "def coords(xbounds, ybounds, zbounds):\n",
    "    return np.stack(np.mgrid[xbounds[0]:xbounds[1]+1, ybounds[0]:ybounds[1]+1, zbounds[0]:zbounds[1]+1], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_values = b_bottom.reshape(-1, 3)\n",
    "bottom_values = np.double(bottom_values)\n",
    "bottom_coords = coords((0, Nx-1), (0, Ny-1), (0, 0)).reshape(-1, 3)\n",
    "bottom_coords = np.double(bottom_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# top_lateral_coords = [coords((0, Nx-1), (0, Ny-1), (Nz-2, Nz)),\n",
    "#                       coords((-1, 1), (0, Ny-1), (0, Nz-1)),\n",
    "#                       coords((Nx-2, Nx), (0, Ny-1), (0, Nz-1)),\n",
    "#                       coords((0, Nx-1), (-1, 1), (0, Nz-1)),\n",
    "#                       coords((0, Nx-1), (Ny-2, Ny), (0, Nz-1))]\n",
    "\n",
    "# r_top_lateral_shape = [c.shape[:-1] for c in top_lateral_coords]\n",
    "# r_top_lateral = np.concatenate([c.reshape(((-1, 3))) for c in top_lateral_coords])\n",
    "# r_bottom = bottom_coords\n",
    "# bz_bottom = bottom_values[:, 2]\n",
    "\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# class PotentialModel(nn.Module):\n",
    "\n",
    "#     def __init__(self, b_n, r_p):\n",
    "#         super().__init__()\n",
    "#         self.register_buffer('b_n', b_n)\n",
    "#         self.register_buffer('r_p', r_p)\n",
    "#         c = np.array([[0, 0, 1/np.sqrt(2*np.pi)]])\n",
    "#         c = torch.tensor(c, dtype=torch.float64)\n",
    "#         self.register_buffer('c', c)\n",
    "\n",
    "#     def forward(self, r):\n",
    "#         numerator = self.b_n[:, None]\n",
    "#         denominator = torch.sqrt(torch.sum((r[None, :] - self.r_p[:, None] + self.c[None])**2, -1))\n",
    "#         potential = torch.sum(numerator/denominator, 0) / (2*np.pi)\n",
    "#         return potential\n",
    "    \n",
    "# pf_batch_size = r_top_lateral.shape[0] // 100\n",
    "# with torch.no_grad():\n",
    "#     b_n = torch.tensor(bz_bottom, dtype=torch.float64)\n",
    "#     r_p = torch.tensor(r_bottom, dtype=torch.float64)\n",
    "#     model = nn.DataParallel(PotentialModel(b_n, r_p)).to(device)\n",
    "\n",
    "#     r_coords= torch.tensor(r_top_lateral, dtype=torch.float64)\n",
    "\n",
    "#     potential = []\n",
    "#     for r, in tqdm(DataLoader(TensorDataset(r_coords), batch_size=pf_batch_size, num_workers=2),\n",
    "#                         desc='Potential Boundary'):\n",
    "#         r = r.to(device)\n",
    "#         p_batch = model(r)\n",
    "#         potential += [p_batch.cpu()]\n",
    "\n",
    "# potential = torch.cat(potential).numpy()\n",
    "\n",
    "# idx = 0\n",
    "# fields = []\n",
    "# for s in r_top_lateral_shape:\n",
    "#     p = potential[idx:idx + np.prod(s)].reshape(s)\n",
    "#     b = - 1 * np.stack(np.gradient(p, axis=[0, 1, 2], edge_order=2), axis=-1)\n",
    "#     fields += [b]\n",
    "#     idx += np.prod(s)\n",
    "\n",
    "# pf_fields = [fields[0][:, :, 1].reshape((-1, 3)),\n",
    "#             fields[1][:, 1, :].reshape((-1, 3)), fields[2][:, 1, :].reshape((-1, 3)),\n",
    "#             fields[3][1, :, :].reshape((-1, 3)), fields[4][1, :, :].reshape((-1, 3))]\n",
    "# pf_coords = [top_lateral_coords[0][:, :, 1].reshape((-1, 3)),\n",
    "#             top_lateral_coords[1][:, 1, :].reshape((-1, 3)), top_lateral_coords[2][:, 1, :].reshape((-1, 3)),\n",
    "#             top_lateral_coords[3][1, :, :].reshape((-1, 3)), top_lateral_coords[4][1, :, :].reshape((-1, 3))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Potential Boundary: 100%|██████████| 11/11 [00:00<00:00, 85.85it/s]\n",
      "Potential Boundary: 100%|██████████| 11/11 [00:00<00:00, 85.04it/s]\n",
      "Potential Boundary: 100%|██████████| 11/11 [00:00<00:00, 95.80it/s]\n",
      "Potential Boundary: 100%|██████████| 11/11 [00:00<00:00, 92.30it/s]\n",
      "Potential Boundary: 100%|██████████| 11/11 [00:00<00:00, 94.84it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class PotentialModel(nn.Module):\n",
    "\n",
    "    def __init__(self, b_n, r_p):\n",
    "        super().__init__()\n",
    "        self.register_buffer('b_n', b_n)\n",
    "        self.register_buffer('r_p', r_p)\n",
    "        c = np.array([[0, 0, 1/np.sqrt(2*np.pi)]])\n",
    "        c = torch.tensor(c, dtype=torch.float64)\n",
    "        self.register_buffer('c', c)\n",
    "\n",
    "    def forward(self, r):\n",
    "        numerator = self.b_n[:, None]\n",
    "        denominator = torch.sqrt(torch.sum((r[None, :] - self.r_p[:, None] + self.c[None])**2, -1))\n",
    "        potential = torch.sum(numerator/denominator, 0) / (2*np.pi)\n",
    "        return potential\n",
    "\n",
    "\n",
    "top_lateral_coordinates = [coords((0, Nx-1), (0, Ny-1), (Nz-1, Nz-1)).reshape(-1, 3),\n",
    "                      coords((0, 0), (0, Ny-1), (0, Nz-1)).reshape(-1, 3),\n",
    "                      coords((Nx-1, Nx-1), (0, Ny-1), (0, Nz-1)).reshape(-1, 3),\n",
    "                      coords((0, Nx-1), (0, 0), (0, Nz-1)).reshape(-1, 3),\n",
    "                      coords((0, Nx-1), (Ny-1, Ny-1), (0, Nz-1)).reshape(-1, 3)]\n",
    "\n",
    "b_n = torch.tensor(bottom_values[:, 2], dtype=torch.float64)\n",
    "r_p = torch.tensor(bottom_coords, dtype=torch.float64)\n",
    "\n",
    "model = nn.DataParallel(PotentialModel(b_n, r_p)).to(device)\n",
    "\n",
    "pf_fields = []\n",
    "pf_coords = []\n",
    "for r_coords in top_lateral_coordinates:\n",
    "    r_coords = torch.tensor(r_coords, dtype=torch.float64)\n",
    "    pf_batch_size = int(np.prod(r_coords.shape[:-1]) // 10)\n",
    "\n",
    "    fields = []\n",
    "    for r, in tqdm(DataLoader(TensorDataset(r_coords), batch_size=pf_batch_size, num_workers=2),\n",
    "                        desc='Potential Boundary'):\n",
    "        r = r.to(device).requires_grad_(True)\n",
    "        p_batch = model(r)\n",
    "        b_p = -1 * torch.autograd.grad(p_batch, r, torch.ones_like(p_batch), retain_graph=True, create_graph=True)[0]\n",
    "        fields += [b_p.clone().detach().cpu().numpy()]\n",
    "    pf_fields += [np.concatenate(fields)]\n",
    "    pf_coords += [r_coords.clone().detach().cpu().numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# class PotentialModel(nn.Module):\n",
    "\n",
    "#     def __init__(self, b_n, r_p):\n",
    "#         super().__init__()\n",
    "#         self.register_buffer('b_n', b_n)\n",
    "#         self.register_buffer('r_p', r_p)\n",
    "#         c = np.array([[0, 0, 1/np.sqrt(2*np.pi)]])\n",
    "#         c = torch.tensor(c, dtype=torch.float64)\n",
    "#         self.register_buffer('c', c)\n",
    "\n",
    "#     def forward(self, r):\n",
    "#         numerator = self.b_n[:, None]\n",
    "#         denominator = torch.sqrt(torch.sum((r[None, :] - self.r_p[:, None] + self.c[None])**2, -1))\n",
    "#         potential = torch.sum(numerator/denominator, 0) / (2*np.pi)\n",
    "#         return potential\n",
    "\n",
    "\n",
    "# top_lateral_coordinates = [coords((0, Nx-1), (0, Ny-1), (Nz-1, Nz-1)),\n",
    "#                            coords((0, 0), (0, Ny-1), (0, Nz-1)),\n",
    "#                            coords((Nx-1, Nx-1), (0, Ny-1), (0, Nz-1)),\n",
    "#                            coords((0, Nx-1), (0, 0), (0, Nz-1)),\n",
    "#                            coords((0, Nx-1), (Ny-1, Ny-1), (0, Nz-1))]\n",
    "\n",
    "# top_lateral_coordinate_shape = [np.prod(c.shape[:-1]) for c in top_lateral_coordinates]\n",
    "# top_lateral_coordinate = np.concatenate([c.reshape(-1, 3) for c in top_lateral_coordinates])\n",
    "\n",
    "# b_n = torch.tensor(bottom_values[:, 2], dtype=torch.float64)\n",
    "# r_p = torch.tensor(bottom_coords, dtype=torch.float64)\n",
    "\n",
    "# model = nn.DataParallel(PotentialModel(b_n, r_p)).to(device)\n",
    "\n",
    "# pf_fields = []\n",
    "# pf_coords = []\n",
    "# for r_coords in top_lateral_coordinate:\n",
    "#     r_coords = torch.tensor(r_coords, dtype=torch.float64)\n",
    "#     pf_batch_size = int(np.prod(r_coords.shape[:-1]) // 10)\n",
    "\n",
    "#     fields = []\n",
    "#     for r, in tqdm(DataLoader(TensorDataset(r_coords), batch_size=pf_batch_size, num_workers=2),\n",
    "#                         desc='Potential Boundary'):\n",
    "#         r = r.to(device).requires_grad_(True)\n",
    "#         p_batch = model(r)\n",
    "#         b_p = -1 * torch.autograd.grad(p_batch, r, torch.ones_like(p_batch), retain_graph=True, create_graph=True)[0]\n",
    "#         fields += [b_p.clone().detach().cpu().numpy()]\n",
    "#     pf_fields += [np.concatenate(fields)]\n",
    "#     pf_coords += [r_coords.clone().detach().cpu().numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_lateral_values = np.concatenate(pf_fields) \n",
    "top_lateral_coords = np.concatenate(pf_coords)\n",
    "\n",
    "boundary_values = np.concatenate([top_lateral_values, bottom_values])\n",
    "boundary_coords = np.concatenate([top_lateral_coords, bottom_coords])\n",
    "\n",
    "normalized_boundary_values = boundary_values / b_norm\n",
    "normalized_boundary_coords = boundary_coords / spatial_norm\n",
    "\n",
    "boundary_data = np.stack([normalized_boundary_coords, normalized_boundary_values], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BModel(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, num_neurons, num_layers):\n",
    "        super().__init__()\n",
    "        self.d_in = nn.Linear(num_inputs, num_neurons).double()\n",
    "        lin = [nn.Linear(num_neurons, num_neurons).double() for _ in range(num_layers)]\n",
    "        self.linear_layers = nn.ModuleList(lin)\n",
    "        self.d_out = nn.Linear(num_neurons, num_outputs).double()\n",
    "        self.activation = torch.sin\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.d_in(x))\n",
    "        for l in self.linear_layers:\n",
    "            x = self.activation(l(x))\n",
    "        B = self.d_out(x)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BModel(3, 3, 256, 8).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "opt = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "scheduler = ExponentialLR(opt, gamma=(5e-5 / 5e-4) ** (1 / total_iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collocation_coords = coords((0, Nx-1), (0, Ny-1), (0, Nz-1)).reshape(-1, 3)\n",
    "normalized_collocation_coords = collocation_coords / spatial_norm\n",
    "normalized_collocation_coords = torch.tensor(normalized_collocation_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled_boundary_data_index = np.random.permutation(boundary_data.shape[0])\n",
    "# shuffled_boundary_data = boundary_data[shuffled_boundary_data_index]\n",
    "# batch_size = 1000\n",
    "# pad = batch_size - shuffled_boundary_data.shape[0] % batch_size\n",
    "# padded_shuffled_boundary_data = np.concatenate([shuffled_boundary_data, shuffled_boundary_data[:pad]])\n",
    "# n_batches = padded_shuffled_boundary_data.shape[0] // batch_size\n",
    "# boundary_batches = np.array(np.split(padded_shuffled_boundary_data, n_batches), dtype=np.double)\n",
    "# np.save('boundary_batches.npy', boundary_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'results/run'\n",
    "os.makedirs(base_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "log = logging.getLogger()\n",
    "log.setLevel(logging.INFO)\n",
    "for hdlr in log.handlers[:]:  # remove all old handlers\n",
    "    log.removeHandler(hdlr)\n",
    "log.addHandler(logging.FileHandler(\"{0}/{1}.log\".format(base_path, \"info_log\")))  # set the new file handler\n",
    "log.addHandler(logging.StreamHandler())  # set the new console handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import RandomSampler, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundaryDataset(Dataset):\n",
    "    def __init__(self, batches_path):\n",
    "        self.batches_path = batches_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return np.load(self.batches_path, mmap_mode='r').shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # lazy load data\n",
    "        d = np.load(self.batches_path, mmap_mode='r')[idx]\n",
    "        d = np.copy(d)\n",
    "        coord, field = d[:, 0],  d[:, 1]\n",
    "        return coord, field\n",
    "    \n",
    "def init_loader(batch_size, boundary_data, num_workers, iterations):\n",
    "    # shuffle boundary_data\n",
    "    r = np.random.permutation(boundary_data.shape[0])\n",
    "    boundary_data = boundary_data[r]\n",
    "    # adjust to batch size\n",
    "    pad = batch_size - boundary_data.shape[0] % batch_size\n",
    "    boundary_data = np.concatenate([boundary_data, boundary_data[:pad]])\n",
    "    # split boundary_data into batches\n",
    "    n_batches = boundary_data.shape[0] // batch_size\n",
    "    batches = np.array(np.split(boundary_data, n_batches), dtype=np.double)\n",
    "    # store batches to disk\n",
    "    batches_path = os.path.join(base_path, 'batches.npy')\n",
    "    np.save(batches_path, batches)\n",
    "    # create boundary_data loaders\n",
    "    dataset = BoundaryDataset(batches_path)\n",
    "    # create loader\n",
    "    data_loader = DataLoader(dataset, batch_size=None, num_workers=num_workers, pin_memory=True,\n",
    "                                sampler=RandomSampler(dataset, replacement=True, num_samples=iterations))\n",
    "    return data_loader, batches_path\n",
    "\n",
    "class RandomCoordinateSampler():\n",
    "\n",
    "    def __init__(self, cube_shape, spatial_norm, batch_size, cuda=True):\n",
    "        self.cube_shape = cube_shape\n",
    "        self.spatial_norm = spatial_norm\n",
    "        self.batch_size = batch_size\n",
    "        self.float_tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "    def load_sample(self):\n",
    "        random_coords = self.float_tensor(self.batch_size, 3).uniform_()\n",
    "        random_coords[:, 0] *= self.cube_shape[0] / self.spatial_norm\n",
    "        random_coords[:, 1] *= self.cube_shape[1] / self.spatial_norm\n",
    "        random_coords[:, 2] *= self.cube_shape[2] / self.spatial_norm\n",
    "        return random_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using device: cuda (gpus 1) ['NVIDIA GeForce RTX 3060']\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda import get_device_name\n",
    "n_gpus = torch.cuda.device_count()\n",
    "device_names = [get_device_name(i) for i in range(n_gpus)]\n",
    "log.info('Using device: %s (gpus %d) %s' % (str(device), n_gpus, str(device_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian(output, coords):\n",
    "    jac_matrix = [torch.autograd.grad(output[:, i], coords,\n",
    "                                      grad_outputs=torch.ones_like(output[:, i]).to(output),\n",
    "                                      retain_graph=True,\n",
    "                                      create_graph=True)[0]\n",
    "                  for i in range(output.shape[1])]\n",
    "    jac_matrix = torch.stack(jac_matrix, dim=1)\n",
    "    return jac_matrix\n",
    "\n",
    "def calculate_loss(b, coords):\n",
    "    jac_matrix = jacobian(b, coords)\n",
    "    dBx_dx = jac_matrix[:, 0, 0]\n",
    "    dBy_dx = jac_matrix[:, 1, 0]\n",
    "    dBz_dx = jac_matrix[:, 2, 0]\n",
    "    dBx_dy = jac_matrix[:, 0, 1]\n",
    "    dBy_dy = jac_matrix[:, 1, 1]\n",
    "    dBz_dy = jac_matrix[:, 2, 1]\n",
    "    dBx_dz = jac_matrix[:, 0, 2]\n",
    "    dBy_dz = jac_matrix[:, 1, 2]\n",
    "    dBz_dz = jac_matrix[:, 2, 2]\n",
    "    #\n",
    "    rot_x = dBz_dy - dBy_dz\n",
    "    rot_y = dBx_dz - dBz_dx\n",
    "    rot_z = dBy_dx - dBx_dy\n",
    "    #\n",
    "    j = torch.stack([rot_x, rot_y, rot_z], -1)\n",
    "    jxb = torch.cross(j, b, -1)\n",
    "    force_loss = torch.sum(jxb ** 2, dim=-1) / (torch.sum(b ** 2, dim=-1) + 1e-7)\n",
    "    divergence_loss = (dBx_dx + dBy_dy + dBz_dz) ** 2\n",
    "    return divergence_loss, force_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "# batch_size = int(np.prod(boundary_data.shape[0]) // 10)\n",
    "batch_size = 10000\n",
    "# pad = batch_size - boundary_data.shape[0] % batch_size\n",
    "# padded_boundary_data = np.concatenate([boundary_data, boundary_data[:pad]])\n",
    "# padded_boundary_data = torch.tensor(padded_boundary_data)\n",
    "# dataset = TensorDataset(padded_boundary_data)\n",
    "# data_loader = DataLoader(dataset, batch_size=batch_size, num_workers=2, shuffle=True)\n",
    "\n",
    "data_loader, batches_path = init_loader(batch_size, boundary_data, 4, total_iterations)\n",
    "\n",
    "sampler = RandomCoordinateSampler(cube_shape, spatial_norm, batch_size * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 5/50000 [00:17<47:41:37,  3.43s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     60\u001b[0m \u001b[39m# update step\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mclip_grad_norm_(model\u001b[39m.\u001b[39;49mparameters(), \u001b[39m0.1\u001b[39;49m)\n\u001b[1;32m     62\u001b[0m opt\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     64\u001b[0m \u001b[39m# save loss information\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:76\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mfor\u001b[39;00m ((device, _), [grads]) \u001b[39min\u001b[39;00m grouped_grads\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m (foreach \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m foreach) \u001b[39mand\u001b[39;00m _has_foreach_support(grads, device\u001b[39m=\u001b[39mdevice):\n\u001b[0;32m---> 76\u001b[0m         torch\u001b[39m.\u001b[39;49m_foreach_mul_(grads, clip_coef_clamped\u001b[39m.\u001b[39;49mto(device))  \u001b[39m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[39melif\u001b[39;00m foreach:\n\u001b[1;32m     78\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mforeach=True was passed, but can\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mt use the foreach API on \u001b[39m\u001b[39m{\u001b[39;00mdevice\u001b[39m.\u001b[39mtype\u001b[39m}\u001b[39;00m\u001b[39m tensors\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_b_diff = []\n",
    "total_divergence_loss = []\n",
    "total_force_loss = []\n",
    "model.train()\n",
    "for iter, (boundary_coords, b_true) in tqdm(enumerate(data_loader, start=0),\n",
    "                                                    total=len(data_loader), desc='Training'):\n",
    "    opt.zero_grad()\n",
    "    # load input data\n",
    "    boundary_coords, b_true= boundary_coords.to(device), b_true.to(device)\n",
    "    random_coords = sampler.load_sample()\n",
    "\n",
    "    # concatenate boundary and random points\n",
    "    n_boundary_coords = boundary_coords.shape[0]\n",
    "    coords = torch.cat([boundary_coords, random_coords], 0)\n",
    "    coords.requires_grad = True\n",
    "    \n",
    "    # forward step\n",
    "    b = model(coords)\n",
    "\n",
    "    # if iter == 0:\n",
    "    #     model.eval()\n",
    "    #     torch.save({'model': model,\n",
    "    #         'cube_shape': cube_shape,\n",
    "    #         'b_norm': b_norm,\n",
    "    #         'spatial_norm': spatial_norm,\n",
    "    #         'meta_info': meta_info}, os.path.join(base_path, 'fields_%06d.nf2' % iter))\n",
    "    #     plot_sample(iter-1, batch_size=batch_size)\n",
    "    #     model.train()\n",
    "\n",
    "    # compute boundary loss\n",
    "    boundary_b = b[:n_boundary_coords]\n",
    "    b_diff = torch.abs(boundary_b - b_true)\n",
    "    b_diff = torch.mean(b_diff.pow(2).sum(-1))\n",
    "\n",
    "    # compute div and ff loss\n",
    "    divergence_loss, force_loss = calculate_loss(b, coords)\n",
    "\n",
    "    # reset grad from auto-gradient operation\n",
    "    opt.zero_grad()\n",
    "    # compute loss\n",
    "    (b_diff * w_bc +\n",
    "        divergence_loss.mean() * w_div +\n",
    "        force_loss.mean() * w_ff).backward()\n",
    "    \n",
    "    if iter == 0:\n",
    "        model.eval()\n",
    "        torch.save({'BC_loss': b_diff.detach().cpu().numpy(),\n",
    "            'lambda_BC': w_bc,\n",
    "            'divergence_loss': divergence_loss.mean().detach().cpu().numpy(),\n",
    "            'lambda_div': w_div,\n",
    "            'force_loss': force_loss.mean().detach().cpu().numpy(),\n",
    "            'lambda_ff': w_ff,}, os.path.join(base_path, 'loss_%06d.nf2' % iter))\n",
    "        torch.save({'model': model,\n",
    "            'cube_shape': cube_shape,\n",
    "            'b_norm': b_norm,\n",
    "            'spatial_norm': spatial_norm,\n",
    "            'meta_info': meta_info}, os.path.join(base_path, 'fields_%06d.nf2' % iter))\n",
    "        model.train()\n",
    "\n",
    "    # update step\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "    opt.step()\n",
    "\n",
    "    # save loss information\n",
    "    total_b_diff += [b_diff.detach().cpu().numpy()]\n",
    "    total_divergence_loss += [divergence_loss.mean().detach().cpu().numpy()]\n",
    "    total_force_loss += [force_loss.mean().detach().cpu().numpy()]\n",
    "\n",
    "    # update training parameters\n",
    "    if w_bc > 1:\n",
    "        w_bc *= w_bc_decay\n",
    "    if scheduler.get_last_lr()[0] > 5e-5:\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39miter\u001b[39m, (bc_coords, bc_values) \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(data_loader, start\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(data_loader), desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[39m# bc_data = bc_data[0]\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[39m# bc_coords = bc_data[:, 0, :].to(device)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[39m# bc_values = bc_data[:, 1, :].to(device)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     bc_coords \u001b[39m=\u001b[39m bc_coords\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for iter, (bc_coords, bc_values) in tqdm(enumerate(data_loader, start=0), total=len(data_loader), desc='Training'):\n",
    "    # bc_data = bc_data[0]\n",
    "    # bc_coords = bc_data[:, 0, :].to(device)\n",
    "    # bc_values = bc_data[:, 1, :].to(device)\n",
    "    bc_coords = bc_coords.to(device)\n",
    "    bc_values = bc_values.to(device)\n",
    "\n",
    "    perm = torch.randperm(normalized_collocation_coords.shape[0])\n",
    "    idx = perm[:batch_size]\n",
    "    co_coords = normalized_collocation_coords[idx].to(device)\n",
    "\n",
    "    r = torch.cat([bc_coords, co_coords], 0)\n",
    "    r.requires_grad = True \n",
    "\n",
    "    B = model(r)\n",
    "    bc_coord = r[0:bc_coords.shape[0]]\n",
    "    bc_B = B[0:bc_coords.shape[0]]\n",
    "\n",
    "    bc_loss = torch.sum((bc_B - bc_values)**2, dim=-1)\n",
    "    bc_loss = torch.mean(bc_loss)\n",
    "\n",
    "    co_coord = r[bc_coords.shape[0]:]\n",
    "    co_B = B[bc_coords.shape[0]:]\n",
    "\n",
    "    dBx_dr = torch.autograd.grad(co_B[:, 0], co_coord, torch.ones_like(co_B[:, 0]), retain_graph=True, create_graph=True)[0]\n",
    "    dBy_dr = torch.autograd.grad(co_B[:, 1], co_coord, torch.ones_like(co_B[:, 1]), retain_graph=True, create_graph=True)[0]\n",
    "    dBz_dr = torch.autograd.grad(co_B[:, 2], co_coord, torch.ones_like(co_B[:, 2]), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "    dBx_dx = dBx_dr[:, 0]\n",
    "    dBx_dy = dBx_dr[:, 1]\n",
    "    dBx_dz = dBx_dr[:, 2]\n",
    "\n",
    "    dBy_dx = dBy_dr[:, 0]\n",
    "    dBy_dy = dBy_dr[:, 1]\n",
    "    dBy_dz = dBy_dr[:, 2]\n",
    "\n",
    "    dBz_dx = dBz_dr[:, 0]\n",
    "    dBz_dy = dBz_dr[:, 1]\n",
    "    dBz_dz = dBz_dr[:, 2]\n",
    "\n",
    "    rot_x = dBz_dy - dBy_dz\n",
    "    rot_y = dBx_dz - dBz_dx\n",
    "    rot_z = dBy_dx - dBx_dy\n",
    "\n",
    "    J = torch.stack([rot_x, rot_y, rot_z], -1)\n",
    "    JxB = torch.cross(J, B, dim=-1)\n",
    "\n",
    "    divB = dBx_dx + dBy_dy + dBz_dz\n",
    "\n",
    "    force_free_loss = torch.sum(JxB**2, dim=-1) / (torch.sum(B**2, dim=-1) + 1e-7)\n",
    "    force_free_loss = torch.mean(force_free_loss)\n",
    "    divergence_loss = torch.sum((divB)**2, dim=-1)\n",
    "    divergence_loss = torch.mean(divergence_loss)\n",
    "\n",
    "    loss = w_bc*bc_loss + w_ff*force_free_loss + w_div*divergence_loss\n",
    "\n",
    "    if iter == 0:\n",
    "        model.eval()\n",
    "        torch.save({'BC_loss': bc_loss.detach().cpu().numpy(),\n",
    "                    'lambda_BC': w_bc,\n",
    "                    'divergence_loss': divergence_loss.detach().cpu().numpy(),\n",
    "                    'lambda_div': w_div,\n",
    "                    'force_loss': force_free_loss.detach().cpu().numpy(),\n",
    "                    'lambda_ff': w_ff,\n",
    "                    'LR':scheduler.get_last_lr()[0]}, \n",
    "                    os.path.join(base_path, 'loss_%06d.nf2' % iter))\n",
    "        torch.save({'model': model,\n",
    "                    'cube_shape': cube_shape,\n",
    "                    'b_norm': b_norm,\n",
    "                    'spatial_norm': spatial_norm,\n",
    "                    'meta_info': meta_info}, \n",
    "                    os.path.join(base_path, 'fields_%06d.nf2' % iter))\n",
    "        model.train()\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "    opt.step()\n",
    "\n",
    "    if (log_interval > 0 and (iter + 1) % log_interval == 0) or (iter == 0):\n",
    "        # log loss\n",
    "        log.info('[Iteration %06d/%06d] [loss: %.08f] [bc_loss: %.08f; div_loss: %.08f; ff_loss: %.08f] [w_bc: %f, LR: %f] [%s]' %\n",
    "                        (iter + 1, total_iterations,\n",
    "                        loss,\n",
    "                        w_bc*bc_loss,\n",
    "                        w_ff*force_free_loss,\n",
    "                        w_div*divergence_loss,\n",
    "                        w_bc,\n",
    "                        scheduler.get_last_lr()[0],\n",
    "                        datetime.now() - start_time))\n",
    "        \n",
    "        model.eval()\n",
    "        torch.save({'BC_loss': bc_loss.detach().cpu().numpy(),\n",
    "                    'lambda_BC': w_bc,\n",
    "                    'divergence_loss': divergence_loss.detach().cpu().numpy(),\n",
    "                    'lambda_div': w_div,\n",
    "                    'force_loss': force_free_loss.detach().cpu().numpy(),\n",
    "                    'lambda_ff': w_ff,\n",
    "                    'LR':scheduler.get_last_lr()[0]}, \n",
    "                    os.path.join(base_path, 'loss_%06d.nf2' % iter))\n",
    "        torch.save({'model': model,\n",
    "                    'cube_shape': cube_shape,\n",
    "                    'b_norm': b_norm,\n",
    "                    'spatial_norm': spatial_norm,\n",
    "                    'meta_info': meta_info}, \n",
    "                    os.path.join(base_path, 'fields_%06d.nf2' % iter))\n",
    "        model.train()\n",
    "\n",
    "    if w_bc > 1:\n",
    "        w_bc *= w_bc*w_bc_decay\n",
    "    if scheduler.get_last_lr()[0] > 5e-5:\n",
    "        scheduler.step()\n",
    "\n",
    "model.eval()\n",
    "torch.save({'BC_loss': bc_loss.detach().cpu().numpy(),\n",
    "            'lambda_BC': w_bc,\n",
    "            'divergence_loss': divergence_loss.detach().cpu().numpy(),\n",
    "            'lambda_div': w_div,\n",
    "            'force_loss': force_free_loss.detach().cpu().numpy(),\n",
    "            'lambda_ff': w_ff,\n",
    "            'LR':scheduler.get_last_lr()[0]}, \n",
    "            os.path.join(base_path, 'loss_final.nf2'))\n",
    "torch.save({'model': model,\n",
    "            'cube_shape': cube_shape,\n",
    "            'b_norm': b_norm,\n",
    "            'spatial_norm': spatial_norm,\n",
    "            'meta_info': meta_info}, \n",
    "            os.path.join(base_path, 'fields_final.nf2'))\n",
    "torch.save({'m': model.state_dict(),\n",
    "            'o': opt.state_dict(), },\n",
    "            os.path.join(base_path, 'model_final.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
