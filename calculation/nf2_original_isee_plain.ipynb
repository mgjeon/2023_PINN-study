{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import netCDF4\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "from datetime import datetime\n",
    "from astropy.nddata import block_reduce\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda import get_device_name\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nlfff:\n",
    "\n",
    "      def __init__(self,filename):\n",
    "            self.filename=filename\n",
    "\n",
    "            nc=netCDF4.Dataset(self.filename,'r')\n",
    "            self.NOAA=nc.NOAA\n",
    "            self.year_month_day_time=nc.year_month_day_time\n",
    "            self.project=nc.project\n",
    "            self.production_date=nc.production_date\n",
    "            self.version=nc.version\n",
    "            self.data_doi=nc.data_doi\n",
    "            self.http_link=nc.http_link\n",
    "            self.Distributor=nc.Distributor\n",
    "            \n",
    "            nc_x=nc.variables['x']\n",
    "            self.x=nc_x[:]\n",
    "            print(nc_x.long_name,' unit:',nc_x.units)\n",
    "            nc_y=nc.variables['y']\n",
    "            self.y=nc_y[:]\n",
    "            print(nc_y.long_name,' unit:',nc_y.units)\n",
    "            nc_z=nc.variables['z']\n",
    "            self.z=nc_z[:]\n",
    "            print(nc_z.long_name,' unit:',nc_z.units)\n",
    "            \n",
    "            nc_bx=nc.variables['Bx']\n",
    "            self.bx=nc_bx[:].transpose(2,1,0)\n",
    "            print(nc_bx.long_name,' unit:',nc_bx.units)\n",
    "            nc_by=nc.variables['By']\n",
    "            self.by=nc_by[:].transpose(2,1,0)\n",
    "            print(nc_by.long_name,' unit:',nc_by.units)\n",
    "            nc_bz=nc.variables['Bz']\n",
    "            self.bz=nc_bz[:].transpose(2,1,0)\n",
    "            print(nc_bz.long_name,' unit:',nc_bz.units)\n",
    "            \n",
    "            nc_bxp=nc.variables['Bx_pot']\n",
    "            self.bx_pot=nc_bxp[:].transpose(2,1,0)\n",
    "            print(nc_bxp.long_name,' unit:',nc_bxp.units)\n",
    "            nc_byp=nc.variables['By_pot']\n",
    "            self.by_pot=nc_byp[:].transpose(2,1,0)\n",
    "            print(nc_byp.long_name,' unit:',nc_byp.units)\n",
    "            nc_bzp=nc.variables['Bz_pot']\n",
    "            self.bz_pot=nc_bzp[:].transpose(2,1,0)\n",
    "            print(nc_bzp.long_name,' unit:',nc_bzp.units)\n",
    "            \n",
    "      def info(self):\n",
    "            self.Lx_Mm=max(self.x) - min(self.x)\n",
    "            self.Ly_Mm=max(self.y) - min(self.y)\n",
    "            print(f'(Lx, Ly) in Mm = ({self.Lx_Mm:.2f}, {self.Ly_Mm:.2f})\\n')\n",
    "            print(f\"NOAA\",self.NOAA)\n",
    "            print(f'year_month_day_time',self.year_month_day_time)\n",
    "            print(f\"project\",self.project)\n",
    "            print(f\"production_date\",self.production_date)\n",
    "            print(f\"version\",self.version)\n",
    "            print(f\"data_doi\",self.data_doi)\n",
    "            print(f\"http_link\",self.http_link)\n",
    "            print(f\"Distributor\",self.Distributor)\n",
    "\n",
    "      def plot(self):\n",
    "            xs=12.0\n",
    "            ys=4.0\n",
    "\n",
    "            xmin=min(self.x)\n",
    "            xmax=max(self.x)\n",
    "            ymin=min(self.y)\n",
    "            ymax=max(self.y)\n",
    "\n",
    "            plt.close()\n",
    "            fig=plt.figure(figsize=(xs,ys))\n",
    "            ax1=fig.add_axes((0.08,0.35,0.25,0.25*xs/ys*(ymax-ymin)/(xmax-xmin)))\n",
    "            ax2=fig.add_axes((0.4,0.35,0.25,0.25*xs/ys*(ymax-ymin)/(xmax-xmin)))\n",
    "            ax3=fig.add_axes((0.72,0.35,0.25,0.25*xs/ys*(ymax-ymin)/(xmax-xmin)))\n",
    "            cax1=fig.add_axes((0.08,0.15,0.25,0.05))\n",
    "            cax2=fig.add_axes((0.4,0.15,0.25,0.05))\n",
    "            cax3=fig.add_axes((0.72,0.15,0.25,0.05))\n",
    "            \n",
    "            vmin=-3000.0 \n",
    "            vmax=3000.0\n",
    "            \n",
    "            im1=ax1.pcolormesh(self.x,self.y,self.bx[:,:,0].transpose(),vmin=vmin,vmax=vmax,cmap='gist_gray',shading='auto')\n",
    "            im2=ax2.pcolormesh(self.x,self.y,self.by[:,:,0].transpose(),vmin=vmin,vmax=vmax,cmap='gist_gray',shading='auto')\n",
    "            im3=ax3.pcolormesh(self.x,self.y,self.bz[:,:,0].transpose(),vmin=vmin,vmax=vmax,cmap='gist_gray',shading='auto')\n",
    "\n",
    "            cbar1=plt.colorbar(im1,cax=cax1,orientation='horizontal')\n",
    "            cbar2=plt.colorbar(im2,cax=cax2,orientation='horizontal')\n",
    "            cbar3=plt.colorbar(im3,cax=cax3,orientation='horizontal')\n",
    "            \n",
    "            ax1.set_title('Bx [G]')\n",
    "            ax1.set_xlabel('x [Mm]')\n",
    "            ax1.set_ylabel('y [Mm]')\n",
    "            \n",
    "            ax2.set_title('By [G]')\n",
    "            ax2.set_xlabel('x [Mm]')\n",
    "            ax2.set_ylabel('y [Mm]')\n",
    "            \n",
    "            ax3.set_title('Bz [G]')\n",
    "            ax3.set_xlabel('x [Mm]')\n",
    "            ax3.set_ylabel('y [Mm]')\n",
    "            \n",
    "            #plt.pause(0.1)\n",
    "            # plt.savefig('./B.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (westward)  unit: Mm\n",
      "y (northward)  unit: Mm\n",
      "z (out ot photosphere)  unit: Mm\n",
      "Bx (westward)  unit: G\n",
      "By (northward)  unit: G\n",
      "Bz (out of photosphere)  unit: G\n",
      "Bx_pot (westward)  unit: G\n",
      "By_pot (northward)  unit: G\n",
      "Bz_pot (out of photosphere)  unit: G\n"
     ]
    }
   ],
   "source": [
    "# nc_filepath = '/home/tensor/workspace/pinn_study/_data/12673_20170905_202400/12673_20170905_202400.nc'\n",
    "# nc_filepath = '/Users/mgjeon/Workspace/pinn_study/_data/12673_20170905_202400.nc'\n",
    "nc_filepath = '/nas/obsdata/isee_nlfff_v1.2/12673/12673_20170905_202400.nc'\n",
    "data = nlfff(nc_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lx: 250.724242944\n",
      "Ly: 163.262298624\n",
      "Lz: 163.262298624\n",
      "Nx: 513\n",
      "Ny: 257\n",
      "Nz: 257\n",
      "dx: 0.48969578700000227\n",
      "dy: 0.6377433539999942\n",
      "dz: 0.637743354\n",
      "dV: 1.9916739845722572e+23\n",
      "(Nx-1)*dx: 250.72424294400116\n",
      "(Ny-1)*dy: 163.2622986239985\n",
      "(Nz-1)*dz: 163.262298624\n"
     ]
    }
   ],
   "source": [
    "Lx = max(data.x) - min(data.x)\n",
    "Ly = max(data.y) - min(data.y)\n",
    "Lz = max(data.z) - min(data.z)\n",
    "\n",
    "Nx = len(data.x)\n",
    "Ny = len(data.y)\n",
    "Nz = len(data.z)\n",
    "\n",
    "dx = np.diff(data.x)[0]\n",
    "dy = np.diff(data.y)[0]\n",
    "dz = np.diff(data.z)[0]\n",
    "\n",
    "print(f'Lx: {Lx}')\n",
    "print(f'Ly: {Ly}')\n",
    "print(f'Lz: {Lz}')\n",
    "\n",
    "print(f'Nx: {Nx}')\n",
    "print(f'Ny: {Ny}')\n",
    "print(f'Nz: {Nz}')\n",
    "\n",
    "# Mm per pixel\n",
    "print(f'dx: {dx}')\n",
    "print(f'dy: {dy}')\n",
    "print(f'dz: {dz}')\n",
    "\n",
    "# Mm^3 per pixel^3\n",
    "dV = dx*dy*dz \n",
    "# cm^3 per pixel^3\n",
    "dV = dx*dy*dz*(1e8**3)\n",
    "print(f'dV: {dV}')\n",
    "\n",
    "# (Nx-1) : # of pixels in x-direction\n",
    "# (Nx-1)*dx : Mm in x-direction\n",
    "print(f'(Nx-1)*dx: {(Nx-1)*dx}')\n",
    "print(f'(Ny-1)*dy: {(Ny-1)*dy}')\n",
    "print(f'(Nz-1)*dz: {(Nz-1)*dz}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\"\n",
    "\n",
    "# JSON\n",
    "# base_path = '/home/tensor/workspace/pinn_study/_run/isee_spinn'\n",
    "base_path = '/userhome/jeon_mg/workspace/pinn_study/_run/isee_plain'\n",
    "# base_path = '/Users/mgjeon/Workspace/pinn_study/_run/isee_spinn'\n",
    "# work_directory = '/home/tensor/workspace/pinn_study/_run/'\n",
    "work_directory = '/userhome/jeon_mg/workspace/pinn_study/_run/'\n",
    "meta_path = None\n",
    "\n",
    "# d_slice = None\n",
    "bin = 1\n",
    "\n",
    "height = 257\n",
    "spatial_norm = 320\n",
    "b_norm = 2500\n",
    "\n",
    "dim = 256\n",
    "\n",
    "lambda_div = 1e-1\n",
    "lambda_ff = 1e-1\n",
    "decay_iterations = 50000\n",
    "\n",
    "total_iterations = 100000\n",
    "batch_size = 10000\n",
    "log_interval = 10000\n",
    "validation_interval = 10000\n",
    "num_workers = 4\n",
    "\n",
    "meta_info = None\n",
    "positional_encoding = False\n",
    "use_potential_boundary = True\n",
    "potential_strides = 1\n",
    "use_vector_potential = False\n",
    "\n",
    "device = None\n",
    "\n",
    "# init logging\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "log = logging.getLogger()\n",
    "log.setLevel(logging.INFO)\n",
    "for hdlr in log.handlers[:]:  # remove all old handlers\n",
    "    log.removeHandler(hdlr)\n",
    "log.addHandler(logging.FileHandler(\"{0}/{1}.log\".format(base_path, \"info_log\")))  # set the new file handler\n",
    "log.addHandler(logging.StreamHandler())  # set the new console handler\n",
    "\n",
    "start_time = datetime.now()\n",
    "base_path = os.path.join(base_path, 'dim%d_bin%d_pf%s_ld%s_lf%s' % (\n",
    "        dim, bin, str(use_potential_boundary), lambda_div, lambda_ff))\n",
    "\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "b_cube = np.array(np.stack([data.bx[:, :, 0], data.by[:, :, 0], data.bz[:, :, 0]], axis=-1))\n",
    "meta_info = None\n",
    "\n",
    "# if d_slice is not None:\n",
    "#     b_cube = b_cube[d_slice[0]:d_slice[1], d_slice[2]:d_slice[3]]\n",
    "\n",
    "# if bin > 1:\n",
    "#     b_cube = block_reduce(b_cube, (bin, bin, 1), np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_coords = np.stack(np.mgrid[:b_cube.shape[0], :b_cube.shape[1], :1], -1)\n",
    "mf_coords = mf_coords.reshape((-1, 3))\n",
    "mf_values = b_cube.reshape((-1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PotentialModel(nn.Module):\n",
    "\n",
    "    def __init__(self, b_n, r_p):\n",
    "        super().__init__()\n",
    "        self.register_buffer('b_n', b_n)\n",
    "        self.register_buffer('r_p', r_p)\n",
    "        c = np.zeros((1, 3))\n",
    "        c[:, 2] = (1 / np.sqrt(2 * np.pi))\n",
    "        c = torch.tensor(c, dtype=torch.float32, )\n",
    "        self.register_buffer('c', c)\n",
    "\n",
    "    def forward(self, coord):\n",
    "        v1 = self.b_n[:, None]\n",
    "        v2 = 2 * np.pi * ((-self.r_p[:, None] + coord[None, :] + self.c[None]) ** 2).sum(-1) ** 0.5\n",
    "        potential = torch.sum(v1 / v2, dim=0)\n",
    "        return potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Potential Boundary: 100%|██████████| 778/778 [00:33<00:00, 23.15it/s]\n"
     ]
    }
   ],
   "source": [
    "pf_batch_size = int(1024 * 512 ** 2 / np.prod(b_cube.shape[:2]))\n",
    "\n",
    "b_n = b_cube[:, :, 2]\n",
    "cube_shape = (*b_n.shape, height)\n",
    "b_n = b_n.reshape((-1)).astype(np.float32)\n",
    "coords = [np.stack(np.mgrid[:cube_shape[0], :cube_shape[1], cube_shape[2] - 2:cube_shape[2] + 1], -1),\n",
    "            np.stack(np.mgrid[:cube_shape[0], -1:2, :cube_shape[2]], -1),\n",
    "            np.stack(np.mgrid[:cube_shape[0], cube_shape[1] - 2:cube_shape[1] + 1, :cube_shape[2]], -1),\n",
    "            np.stack(np.mgrid[-1:2, :cube_shape[1], :cube_shape[2]], -1),\n",
    "            np.stack(np.mgrid[cube_shape[0] - 2:cube_shape[0] + 1, :cube_shape[1], :cube_shape[2]], -1), ]\n",
    "coords_shape = [c.shape[:-1] for c in coords]\n",
    "flat_coords = np.concatenate([c.reshape(((-1, 3))) for c in coords])\n",
    "\n",
    "r_p = np.stack(np.mgrid[:cube_shape[0], :cube_shape[1], :1], -1).reshape((-1, 3))\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "with torch.no_grad():\n",
    "    b_n = torch.tensor(b_n, dtype=torch.float32, )\n",
    "    r_p = torch.tensor(r_p, dtype=torch.float32, )\n",
    "    model = nn.DataParallel(PotentialModel(b_n, r_p, )).to(device)\n",
    "\n",
    "    flat_coords = torch.tensor(flat_coords, dtype=torch.float32, )\n",
    "\n",
    "    potential = []\n",
    "    for coord, in tqdm(DataLoader(TensorDataset(flat_coords), batch_size=pf_batch_size, num_workers=2),\n",
    "                        desc='Potential Boundary'):\n",
    "        coord = coord.to(device)\n",
    "        p_batch = model(coord)\n",
    "        potential += [p_batch.cpu()]\n",
    "\n",
    "potential = torch.cat(potential).numpy()\n",
    "idx = 0\n",
    "fields = []\n",
    "for s in coords_shape:\n",
    "    p = potential[idx:idx + np.prod(s)].reshape(s)\n",
    "    b = - 1 * np.stack(np.gradient(p, axis=[0, 1, 2], edge_order=2), axis=-1)\n",
    "    fields += [b]\n",
    "    idx += np.prod(s)\n",
    "\n",
    "fields = [fields[0][:, :, 1].reshape((-1, 3)),\n",
    "            fields[1][:, 1, :].reshape((-1, 3)), fields[2][:, 1, :].reshape((-1, 3)),\n",
    "            fields[3][1, :, :].reshape((-1, 3)), fields[4][1, :, :].reshape((-1, 3))]\n",
    "coords = [coords[0][:, :, 1].reshape((-1, 3)),\n",
    "            coords[1][:, 1, :].reshape((-1, 3)), coords[2][:, 1, :].reshape((-1, 3)),\n",
    "            coords[3][1, :, :].reshape((-1, 3)), coords[4][1, :, :].reshape((-1, 3))]\n",
    "\n",
    "pf_coords, pf_values = np.concatenate(coords), np.concatenate(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_values = np.array(pf_values, dtype=np.float32)\n",
    "pf_coords = np.array(pf_coords, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = np.concatenate([pf_coords, mf_coords])\n",
    "values = np.concatenate([pf_values, mf_values])\n",
    "coords = coords.astype(np.float32)\n",
    "values = values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.0869141 ,   0.41046143,  -0.59832   ],\n",
       "       [ -1.0871735 ,   0.41107178,  -0.6021805 ],\n",
       "       [ -1.0874405 ,   0.4116516 ,  -0.6060486 ],\n",
       "       ...,\n",
       "       [ 64.65      , -66.61      ,  13.29      ],\n",
       "       [ 92.42      ,  35.37      , -30.01      ],\n",
       "       [  0.        ,   0.        ,   0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = Normalize(-b_norm, b_norm, clip=False)(values) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_array(\n",
       "  data=[[-0.00043476,  0.00016415, -0.00023937],\n",
       "        [-0.00043488,  0.00016451, -0.00024092],\n",
       "        [-0.00043494,  0.00016463, -0.00024241],\n",
       "        ...,\n",
       "        [ 0.02585995, -0.02664405,  0.00531602],\n",
       "        [ 0.03696799,  0.014148  , -0.01200402],\n",
       "        [ 0.        ,  0.        ,  0.        ]],\n",
       "  mask=False,\n",
       "  fill_value=1e+20,\n",
       "  dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0., 256.],\n",
       "       [  0.,   1., 256.],\n",
       "       [  0.,   2., 256.],\n",
       "       ...,\n",
       "       [512., 254.,   0.],\n",
       "       [512., 255.,   0.],\n",
       "       [512., 256.,   0.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = coords / spatial_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.      , 0.      , 0.8     ],\n",
       "       [0.      , 0.003125, 0.8     ],\n",
       "       [0.      , 0.00625 , 0.8     ],\n",
       "       ...,\n",
       "       [1.6     , 0.79375 , 0.      ],\n",
       "       [1.6     , 0.796875, 0.      ],\n",
       "       [1.6     , 0.8     , 0.      ]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.stack([coords, values], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_shape = [*b_cube.shape[:-1], height]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sine(nn.Module):\n",
    "    def __init__(self, w0=1.):\n",
    "        super().__init__()\n",
    "        self.w0 = w0\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(self.w0 * x)\n",
    "    \n",
    "class BModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_coords, out_values, dim):\n",
    "        super().__init__()\n",
    "        self.d_in = nn.Linear(in_coords, dim)\n",
    "        lin = [nn.Linear(dim, dim) for _ in range(8)]\n",
    "        self.linear_layers = nn.ModuleList(lin)\n",
    "        self.d_out = nn.Linear(dim, out_values)\n",
    "        self.activation = Sine()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.d_in(x))\n",
    "        for l in self.linear_layers:\n",
    "            x = self.activation(l(x))\n",
    "        b = self.d_out(x)\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BModel(3, 3, dim).to(device)\n",
    "parallel_model = nn.DataParallel(model)\n",
    "parallel_model.to(device)\n",
    "opt = torch.optim.Adam(parallel_model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomCoordinateSampler():\n",
    "\n",
    "    def __init__(self, cube_shape, spatial_norm, batch_size, cuda=True):\n",
    "        self.cube_shape = cube_shape\n",
    "        self.spatial_norm = spatial_norm\n",
    "        self.batch_size = batch_size\n",
    "        self.float_tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "    def load_sample(self):\n",
    "        random_coords = self.float_tensor(self.batch_size, 3).uniform_()\n",
    "        random_coords[:, 0] *= self.cube_shape[0] / self.spatial_norm\n",
    "        random_coords[:, 1] *= self.cube_shape[1] / self.spatial_norm\n",
    "        random_coords[:, 2] *= self.cube_shape[2] / self.spatial_norm\n",
    "        return random_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = RandomCoordinateSampler(cube_shape, spatial_norm, batch_size * 2)\n",
    "scheduler = ExponentialLR(opt, gamma=(5e-5 / 5e-4) ** (1 / total_iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundaryDataset(Dataset):\n",
    "\n",
    "    def __init__(self, batches_path):\n",
    "        \"\"\"Data set for lazy loading a pre-batched numpy data array.\n",
    "\n",
    "        :param batches_path: path to the numpy array.\n",
    "        \"\"\"\n",
    "        self.batches_path = batches_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return np.load(self.batches_path, mmap_mode='r').shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # lazy load data\n",
    "        d = np.load(self.batches_path, mmap_mode='r')[idx]\n",
    "        d = np.copy(d)\n",
    "        coord, field= d[:, 0],  d[:, 1]\n",
    "        return coord, field\n",
    "    \n",
    "def _init_loader(batch_size, data, num_workers, iterations):\n",
    "    # shuffle data\n",
    "    r = np.random.permutation(data.shape[0])\n",
    "    data = data[r]\n",
    "    # adjust to batch size\n",
    "    pad = batch_size - data.shape[0] % batch_size\n",
    "    data = np.concatenate([data, data[:pad]])\n",
    "    # split data into batches\n",
    "    n_batches = data.shape[0] // batch_size\n",
    "    batches = np.array(np.split(data, n_batches), dtype=np.float32)\n",
    "    # store batches to disk\n",
    "    batches_path = os.path.join(work_directory, 'batches.npy')\n",
    "    np.save(batches_path, batches)\n",
    "    # create data loaders\n",
    "    dataset = BoundaryDataset(batches_path)\n",
    "    # create loader\n",
    "    data_loader = DataLoader(dataset, batch_size=None, num_workers=num_workers, pin_memory=True,\n",
    "                                sampler=RandomSampler(dataset, replacement=True, num_samples=iterations))\n",
    "    return data_loader, batches_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader, batches_path = _init_loader(batch_size, data, num_workers, total_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BModel(\n",
       "  (d_in): Linear(in_features=3, out_features=256, bias=True)\n",
       "  (linear_layers): ModuleList(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (5): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (7): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (d_out): Linear(in_features=256, out_features=3, bias=True)\n",
       "  (activation): Sine()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_b_diff = []\n",
    "total_divergence_loss = []\n",
    "total_force_loss = []\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian(output, coords):\n",
    "    jac_matrix = [torch.autograd.grad(output[:, i], coords,\n",
    "                                      grad_outputs=torch.ones_like(output[:, i]).to(output),\n",
    "                                      retain_graph=True,\n",
    "                                      create_graph=True)[0]\n",
    "                  for i in range(output.shape[1])]\n",
    "    jac_matrix = torch.stack(jac_matrix, dim=1)\n",
    "    return jac_matrix\n",
    "\n",
    "def calculate_loss(b, coords):\n",
    "    jac_matrix = jacobian(b, coords)\n",
    "    dBx_dx = jac_matrix[:, 0, 0]\n",
    "    dBy_dx = jac_matrix[:, 1, 0]\n",
    "    dBz_dx = jac_matrix[:, 2, 0]\n",
    "    dBx_dy = jac_matrix[:, 0, 1]\n",
    "    dBy_dy = jac_matrix[:, 1, 1]\n",
    "    dBz_dy = jac_matrix[:, 2, 1]\n",
    "    dBx_dz = jac_matrix[:, 0, 2]\n",
    "    dBy_dz = jac_matrix[:, 1, 2]\n",
    "    dBz_dz = jac_matrix[:, 2, 2]\n",
    "    #\n",
    "    rot_x = dBz_dy - dBy_dz\n",
    "    rot_y = dBx_dz - dBz_dx\n",
    "    rot_z = dBy_dx - dBx_dy\n",
    "    #\n",
    "    j = torch.stack([rot_x, rot_y, rot_z], -1)\n",
    "    jxb = torch.cross(j, b, -1)\n",
    "    force_loss = torch.sum(jxb ** 2, dim=-1) / (torch.sum(b ** 2, dim=-1) + 1e-7)\n",
    "    divergence_loss = (dBx_dx + dBy_dy + dBz_dz) ** 2\n",
    "    return divergence_loss, force_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_B = 1000 if decay_iterations else 1\n",
    "lambda_B_decay = (1 / 1000) ** (1 / decay_iterations) if decay_iterations is not None else 1\n",
    "history = {'iteration': [], 'height': [],\n",
    "                       'b_loss': [], 'divergence_loss': [], 'force_loss': [], 'sigma_angle': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(base_path, 'extrapolation_result.nf2')\n",
    "checkpoint_path = os.path.join(base_path, 'checkpoint.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample(iteration, n_samples=10, batch_size=4096):\n",
    "        fig, axs = plt.subplots(3, n_samples, figsize=(n_samples * 4, 12))\n",
    "        heights = np.linspace(0, 1, n_samples) ** 2 * (height - 1)  # more samples from lower heights\n",
    "        imgs = np.array([get_image(h, batch_size) for h in heights])\n",
    "        for i in range(3):\n",
    "            for j in range(10):\n",
    "                v_min_max = np.max(np.abs(imgs[j, ..., i]))\n",
    "                axs[i, j].imshow(imgs[j, ..., i].transpose(), cmap='gray', vmin=-v_min_max, vmax=v_min_max,\n",
    "                                 origin='lower')\n",
    "                axs[i, j].set_axis_off()\n",
    "        for j, h in enumerate(heights):\n",
    "            axs[0, j].set_title('%.01f' % h)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(os.path.join(base_path, '%06d.jpg' % (iteration + 1)))\n",
    "        plt.close(fig)\n",
    "\n",
    "def get_image(z=0, batch_size=4096):\n",
    "    image_loader = DataLoader(ImageDataset([*cube_shape, 3], spatial_norm, z),\n",
    "                                batch_size=batch_size, shuffle=False)\n",
    "    image = []\n",
    "    for coord in image_loader:\n",
    "        coord.requires_grad = True\n",
    "        pred_pix = model(coord.to(device))\n",
    "        image.extend(pred_pix.detach().cpu().numpy())\n",
    "    image = np.array(image).reshape((*cube_shape[:2], 3))\n",
    "    return image\n",
    "\n",
    "def save(iteration):\n",
    "    torch.save({'model': model,\n",
    "                'cube_shape': cube_shape,\n",
    "                'b_norm': b_norm,\n",
    "                'spatial_norm': spatial_norm,\n",
    "                'meta_info': meta_info}, save_path)\n",
    "    torch.save({'iteration': iteration + 1,\n",
    "                'm': model.state_dict(),\n",
    "                'o': opt.state_dict(),\n",
    "                'history': history,\n",
    "                'lambda_B': lambda_B},\n",
    "                checkpoint_path)\n",
    "    \n",
    "def validate(z, batch_size):\n",
    "    b, j, div, coords = get_cube(z, batch_size)\n",
    "    b = b.unsqueeze(0) * b_norm\n",
    "    j = j.unsqueeze(0) * b_norm / spatial_norm\n",
    "    div = div.unsqueeze(0) * b_norm / spatial_norm\n",
    "\n",
    "    norm = b.pow(2).sum(-1).pow(0.5) * j.pow(2).sum(-1).pow(0.5)\n",
    "    angle = torch.cross(j, b, dim=-1).pow(2).sum(-1).pow(0.5) / norm\n",
    "    sig = torch.asin(torch.clip(angle, -1. + 1e-7, 1. - 1e-7)) * (180 / np.pi)\n",
    "    sig = torch.abs(sig)\n",
    "    weighted_sig = np.average(sig.numpy(), weights=j.pow(2).sum(-1).pow(0.5).numpy())\n",
    "\n",
    "    b_diff = torch.abs(b[0, :, :, 0, :] - b_cube)\n",
    "    b_diff = torch.clip(b_diff, 0, None)\n",
    "    b_diff = torch.sqrt((b_diff ** 2).sum(-1))\n",
    "\n",
    "    b_norm = b.pow(2).sum(-1).pow(0.5) + 1e-7\n",
    "    div_loss = div / b_norm\n",
    "    for_loss = torch.cross(j, b, dim=-1).pow(2).sum(-1).pow(0.5) / b_norm\n",
    "\n",
    "    return b_diff.mean().numpy(), torch.mean(div_loss).numpy(), \\\n",
    "            torch.mean(for_loss).numpy(), weighted_sig\n",
    "\n",
    "def get_cube(max_height, batch_size=int(1e4)):\n",
    "    b = []\n",
    "    j = []\n",
    "    div = []\n",
    "\n",
    "    coords = np.stack(np.mgrid[:cube_shape[0], :cube_shape[1], :max_height], -1)\n",
    "    coords = torch.tensor(coords / spatial_norm, dtype=torch.float32)\n",
    "    coords_shape = coords.shape[:-1]\n",
    "    coords = coords.view((-1, 3))\n",
    "    for k in tqdm(range(int(np.ceil(coords.shape[0] / batch_size))), desc='Validation'):\n",
    "        coord = coords[k * batch_size: (k + 1) * batch_size]\n",
    "        coord.requires_grad = True\n",
    "        coord = coord.to(device)\n",
    "        b_batch = model(coord)\n",
    "\n",
    "        jac_matrix = jacobian(b_batch, coord)\n",
    "        dBx_dx = jac_matrix[:, 0, 0]\n",
    "        dBy_dx = jac_matrix[:, 1, 0]\n",
    "        dBz_dx = jac_matrix[:, 2, 0]\n",
    "        dBx_dy = jac_matrix[:, 0, 1]\n",
    "        dBy_dy = jac_matrix[:, 1, 1]\n",
    "        dBz_dy = jac_matrix[:, 2, 1]\n",
    "        dBx_dz = jac_matrix[:, 0, 2]\n",
    "        dBy_dz = jac_matrix[:, 1, 2]\n",
    "        dBz_dz = jac_matrix[:, 2, 2]\n",
    "        #\n",
    "        rot_x = dBz_dy - dBy_dz\n",
    "        rot_y = dBx_dz - dBz_dx\n",
    "        rot_z = dBy_dx - dBx_dy\n",
    "        #\n",
    "        j_batch = torch.stack([rot_x, rot_y, rot_z], -1)\n",
    "        div_batch = torch.abs(dBx_dx + dBy_dy + dBz_dz)\n",
    "        #\n",
    "        b += [b_batch.detach().cpu()]\n",
    "        j += [j_batch.detach().cpu()]\n",
    "        div += [div_batch.detach().cpu()]\n",
    "\n",
    "    b = torch.cat(b, dim=0).view((*coords_shape, 3))\n",
    "    j = torch.cat(j, dim=0).view((*coords_shape, 3))\n",
    "    div = torch.cat(div, dim=0).view(coords_shape)\n",
    "    return b, j, div, coords\n",
    "\n",
    "def plotHistory(self):\n",
    "    history = self.history\n",
    "    plt.figure(figsize=(12, 16))\n",
    "    plt.subplot(411)\n",
    "    plt.plot(history['iteration'], history['b_loss'], label='B')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('B')\n",
    "    plt.subplot(412)\n",
    "    plt.plot(history['iteration'], history['divergence_loss'], label='Divergence')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Divergence')\n",
    "    plt.subplot(413)\n",
    "    plt.plot(history['iteration'], history['force_loss'], label='Force')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Force')\n",
    "    plt.subplot(414)\n",
    "    plt.plot(history['iteration'], history['sigma_angle'], label='Angle')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Angle')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(self.base_path, 'history.jpg'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|▉         | 9999/100000 [11:58<1:47:45, 13.92it/s][Iteration 010000/100000] [B-Field: 0.00232540; Div: 0.16562338; For: 0.34179765] [0:12:38.160740]\n",
      "Lambda B: 251.188643\n",
      "LR: 0.000397\n",
      "Training:  10%|▉         | 9999/100000 [12:05<1:48:45, 13.79it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m save(\u001b[39miter\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[39m# validate and plot\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m mean_b, total_divergence, mean_force, sigma_angle \u001b[39m=\u001b[39m validate(height, batch_size)\n\u001b[1;32m     73\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mValidation [Cube: B: \u001b[39m\u001b[39m%.03f\u001b[39;00m\u001b[39m; Div: \u001b[39m\u001b[39m%.03f\u001b[39;00m\u001b[39m; For: \u001b[39m\u001b[39m%.03f\u001b[39;00m\u001b[39m; Sig: \u001b[39m\u001b[39m%.03f\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m     74\u001b[0m                 (mean_b, total_divergence, mean_force, sigma_angle))\n\u001b[1;32m     75\u001b[0m \u001b[39m#\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 42\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(z, batch_size)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidate\u001b[39m(z, batch_size):\n\u001b[0;32m---> 42\u001b[0m     b, j, div, coords \u001b[39m=\u001b[39m get_cube(z, batch_size)\n\u001b[1;32m     43\u001b[0m     b \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m) \u001b[39m*\u001b[39m b_norm\n\u001b[1;32m     44\u001b[0m     j \u001b[39m=\u001b[39m j\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m) \u001b[39m*\u001b[39m b_norm \u001b[39m/\u001b[39m spatial_norm\n",
      "Cell \u001b[0;32mIn[29], line 69\u001b[0m, in \u001b[0;36mget_cube\u001b[0;34m(max_height, batch_size)\u001b[0m\n\u001b[1;32m     66\u001b[0m j \u001b[39m=\u001b[39m []\n\u001b[1;32m     67\u001b[0m div \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 69\u001b[0m coords \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack(np\u001b[39m.\u001b[39mmgrid[:\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39mcube_shape[\u001b[39m0\u001b[39m], :\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcube_shape[\u001b[39m1\u001b[39m], :max_height], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     70\u001b[0m coords \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(coords \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspatial_norm, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     71\u001b[0m coords_shape \u001b[39m=\u001b[39m coords\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "for iter, (boundary_coords, b_true) in tqdm(enumerate(data_loader, start=0),\n",
    "                                                    total=len(data_loader), desc='Training'):\n",
    "    opt.zero_grad()\n",
    "    # load input data\n",
    "    boundary_coords, b_true= boundary_coords.to(device), b_true.to(device)\n",
    "    random_coords = sampler.load_sample()\n",
    "    random_coords = random_coords.to(device)\n",
    "\n",
    "    # concatenate boundary and random points\n",
    "    n_boundary_coords = boundary_coords.shape[0]\n",
    "    coords = torch.cat([boundary_coords, random_coords], 0)\n",
    "    coords.requires_grad = True\n",
    "\n",
    "    # forward step\n",
    "    b = model(coords)\n",
    "\n",
    "    # compute boundary loss\n",
    "    boundary_b = b[:n_boundary_coords]\n",
    "    b_diff = torch.abs(boundary_b - b_true)\n",
    "    b_diff = torch.mean(b_diff.pow(2).sum(-1))\n",
    "\n",
    "    # compute div and ff loss\n",
    "    divergence_loss, force_loss = calculate_loss(b, coords)\n",
    "\n",
    "    # reset grad from auto-gradient operation\n",
    "    opt.zero_grad()\n",
    "    # compute loss\n",
    "    (b_diff * lambda_B +\n",
    "        divergence_loss.mean() * lambda_div +\n",
    "        force_loss.mean() * lambda_ff).backward()\n",
    "    # update step\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "    opt.step()\n",
    "\n",
    "    # save loss information\n",
    "    total_b_diff += [b_diff.detach().cpu().numpy()]\n",
    "    total_divergence_loss += [divergence_loss.mean().detach().cpu().numpy()]\n",
    "    total_force_loss += [force_loss.mean().detach().cpu().numpy()]\n",
    "\n",
    "    # update training parameters\n",
    "    if lambda_B > 1:\n",
    "        lambda_B *= lambda_B_decay\n",
    "    if scheduler.get_last_lr()[0] > 5e-5:\n",
    "        scheduler.step()\n",
    "    # logging\n",
    "    if log_interval > 0 and (iter + 1) % log_interval == 0:\n",
    "        # log loss\n",
    "        logging.info('[Iteration %06d/%06d] [B-Field: %.08f; Div: %.08f; For: %.08f] [%s]' %\n",
    "                        (iter + 1, total_iterations,\n",
    "                        np.mean(total_b_diff),\n",
    "                        np.mean(total_divergence_loss),\n",
    "                        np.mean(total_force_loss),\n",
    "                        datetime.now() - start_time))\n",
    "        # reset\n",
    "        total_b_diff = []\n",
    "        total_divergence_loss = []\n",
    "        total_force_loss = []\n",
    "\n",
    "        # plot sample\n",
    "        model.eval()\n",
    "        plot_sample(iter, batch_size=batch_size)\n",
    "        model.train()\n",
    "\n",
    "        # log decay parameters\n",
    "        logging.info('Lambda B: %f' % (lambda_B))\n",
    "        logging.info('LR: %f' % (scheduler.get_last_lr()[0]))\n",
    "    # validation\n",
    "    if validation_interval > 0 and (iter + 1) % validation_interval == 0:\n",
    "        model.eval()\n",
    "        save(iter)\n",
    "        # validate and plot\n",
    "        mean_b, total_divergence, mean_force, sigma_angle = validate(height, batch_size)\n",
    "        logging.info('Validation [Cube: B: %.03f; Div: %.03f; For: %.03f; Sig: %.03f]' %\n",
    "                        (mean_b, total_divergence, mean_force, sigma_angle))\n",
    "        #\n",
    "        history['iteration'].append(iter + 1)\n",
    "        history['b_loss'].append(mean_b.mean())\n",
    "        history['divergence_loss'].append(total_divergence)\n",
    "        history['force_loss'].append(mean_force)\n",
    "        history['sigma_angle'].append(sigma_angle)\n",
    "        plotHistory()\n",
    "        #\n",
    "        model.train()\n",
    "# save final model state\n",
    "torch.save({'m': model.state_dict(),\n",
    "            'o': opt.state_dict(), },\n",
    "            os.path.join(base_path, 'final.pt'))\n",
    "torch.save({'model': model,\n",
    "            'cube_shape': cube_shape,\n",
    "            'b_norm': b_norm,\n",
    "            'spatial_norm': spatial_norm,\n",
    "            'meta_info': meta_info}, save_path)\n",
    "# cleanup\n",
    "os.remove(batches_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
