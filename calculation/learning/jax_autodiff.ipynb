{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp \n",
    "from jax import grad, jit, vmap \n",
    "from jax import random \n",
    "\n",
    "key = random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.07065082, dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_tanh = grad(jnp.tanh)\n",
    "grad_tanh(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(-0.13621868, dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(grad(jnp.tanh))(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.25265405, dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(grad(grad(jnp.tanh)))(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 0.5 * (jnp.tanh(x/2) + 1)\n",
    "\n",
    "def predict(W, b, inputs):\n",
    "    return sigmoid(jnp.dot(inputs, W)+b)\n",
    "\n",
    "inputs = jnp.array([[0.52, 1.12,  0.77],\n",
    "                   [0.88, -1.08, 0.15],\n",
    "                   [0.52, 0.06, -1.30],\n",
    "                   [0.74, -2.49, 1.39]])\n",
    "targets = jnp.array([True, True, False, True])\n",
    "\n",
    "def loss(W, b):\n",
    "    preds = predict(W, b, inputs)\n",
    "    label_probs = preds*targets + (1-preds)*(1-targets)\n",
    "    return -jnp.sum(jnp.log(label_probs))\n",
    "\n",
    "key, W_key, b_key = random.split(key, 3)\n",
    "W = random.normal(W_key, (3,))\n",
    "b = random.normal(b_key, ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_grad [-0.16965583 -0.8774644  -1.4901346 ]\n",
      "b_grad -0.29227245\n",
      "W_grad [-0.16965583 -0.8774644  -1.4901346 ]\n",
      "b_grad -0.29227245\n"
     ]
    }
   ],
   "source": [
    "# d(loss)/d(W)\n",
    "W_grad = grad(loss, argnums=0)(W, b)\n",
    "print('W_grad', W_grad)\n",
    "\n",
    "# d(loss)/d(b)\n",
    "b_grad = grad(loss, argnums=1)(W, b)\n",
    "print('b_grad', b_grad)\n",
    "\n",
    "W_grad, b_grad = grad(loss, (0, 1))(W, b)\n",
    "print('W_grad', W_grad)\n",
    "print('b_grad', b_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W': Array([-0.16965583, -0.8774644 , -1.4901346 ], dtype=float32), 'b': Array(-0.29227245, dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "def loss2(params_dict):\n",
    "    preds = predict(params_dict['W'], params_dict['b'], inputs)\n",
    "    label_probs = preds*targets + (1-preds)*(1-targets)\n",
    "    return -jnp.sum(jnp.log(label_probs))\n",
    "\n",
    "print(grad(loss2)({'W': W, 'b': b}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value 3.0519388\n",
      "loss value 3.0519388\n"
     ]
    }
   ],
   "source": [
    "from jax import value_and_grad\n",
    "loss_value, Wb_grad = value_and_grad(loss, (0,1))(W, b)\n",
    "print('loss value', loss_value)\n",
    "print('loss value', loss(W, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W [-0.36838785 -2.275689    0.01144757]\n",
      "b 0.8535516\n",
      "b_grad_numerical -0.29325485\n",
      "b_grad_autodiff -0.29227245\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-4\n",
    "print('W', W)\n",
    "print('b', b)\n",
    "\n",
    "b_grad_numerical = (loss(W, b + eps/2.) - loss(W, b - eps/2.))/eps \n",
    "print('b_grad_numerical', b_grad_numerical)\n",
    "print('b_grad_autodiff', grad(loss, 1)(W, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_grad_numerical -0.2002716\n",
      "W_grad_autodiff -0.19909117\n"
     ]
    }
   ],
   "source": [
    "key, subkey = random.split(key)\n",
    "vec = random.normal(subkey, W.shape)\n",
    "unitvec = vec / jnp.sqrt(jnp.vdot(vec, vec))\n",
    "W_grad_numerical = (loss(W + (eps/2.)*unitvec, b) - loss(W - (eps/2.)*unitvec, b))/eps\n",
    "print('W_grad_numerical', W_grad_numerical)\n",
    "print('W_grad_autodiff', jnp.vdot(grad(loss, 0)(W, b), unitvec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.test_util import check_grads\n",
    "check_grads(loss, (W, b), order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mcheck_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fwd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rev'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Check gradients from automatic differentiation against finite differences.\n",
      "\n",
      "Gradients are only checked in a single randomly chosen direction, which\n",
      "ensures that the finite difference calculation does not become prohibitively\n",
      "expensive even for large input/output spaces.\n",
      "\n",
      "Args:\n",
      "  f: function to check at ``f(*args)``.\n",
      "  args: tuple of argument values.\n",
      "  order: forward and backwards gradients up to this order are checked.\n",
      "  modes: lists of gradient modes to check ('fwd' and/or 'rev').\n",
      "  atol: absolute tolerance for gradient equality.\n",
      "  rtol: relative tolerance for gradient equality.\n",
      "  eps: step size used for finite differences.\n",
      "\n",
      "Raises:\n",
      "  AssertionError: if gradients do not match.\n",
      "\u001b[0;31mFile:\u001b[0m      /opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages/jax/_src/public_test_util.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "check_grads?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hvp(f, x, v):\n",
    "    return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W [-0.36838785 -2.275689    0.01144757]\n",
      "b 0.8535516\n",
      "inputs [[ 0.52  1.12  0.77]\n",
      " [ 0.88 -1.08  0.15]\n",
      " [ 0.52  0.06 -1.3 ]\n",
      " [ 0.74 -2.49  1.39]]\n"
     ]
    }
   ],
   "source": [
    "print('W', W)\n",
    "print('b', b)\n",
    "print('inputs', inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W [-0.36838785 -2.275689    0.01144757]\n",
      "f(W) [0.13262254 0.952067   0.6249393  0.9980987 ]\n",
      "jacfwd took 0.004054069519042969 seconds\n",
      "jacfwd result, with shape (4, 3)\n",
      "[[ 0.05981758  0.12883787  0.08857603]\n",
      " [ 0.04015916 -0.04928625  0.00684531]\n",
      " [ 0.12188288  0.01406341 -0.3047072 ]\n",
      " [ 0.00140431 -0.00472531  0.00263782]]\n",
      "jacrev took 0.0628511905670166 seconds\n",
      "jacrev result, with shape (4, 3)\n",
      "[[ 0.05981757  0.12883787  0.08857603]\n",
      " [ 0.04015916 -0.04928625  0.00684531]\n",
      " [ 0.12188289  0.01406341 -0.3047072 ]\n",
      " [ 0.00140431 -0.00472531  0.00263782]]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from jax import jacfwd, jacrev\n",
    "\n",
    "f = lambda W: predict(W, b, inputs)\n",
    "print('W', W)\n",
    "print('f(W)', f(W))\n",
    "\n",
    "start = time.time()\n",
    "J = jacfwd(f)(W)\n",
    "print(\"jacfwd took\", time.time() - start, \"seconds\")\n",
    "print(\"jacfwd result, with shape\", J.shape)\n",
    "print(J)\n",
    "\n",
    "start = time.time()\n",
    "J = jacrev(f)(W)\n",
    "print(\"jacrev took\", time.time() - start, \"seconds\")\n",
    "print(\"jacrev result, with shape\", J.shape)\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward-mode automatic differentiation is more efficient\n",
    "# for \"tall\" and near-square Jacobian matrices\n",
    "\n",
    "# Reverse-mode automatic differentiation is more efficient\n",
    "# for \"wide\" Jacobian matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian from W to logits is\n",
      "[[ 0.05981757  0.12883787  0.08857603]\n",
      " [ 0.04015916 -0.04928625  0.00684531]\n",
      " [ 0.12188289  0.01406341 -0.3047072 ]\n",
      " [ 0.00140431 -0.00472531  0.00263782]]\n",
      "Jacobian from b to logits is\n",
      "[0.11503381 0.04563541 0.23439017 0.00189771]\n"
     ]
    }
   ],
   "source": [
    "def predict_dict(params, inputs):\n",
    "    return predict(params['W'], params['b'], inputs)\n",
    "\n",
    "J_dict = jacrev(predict_dict)({'W': W, 'b': b}, inputs)\n",
    "for k, v in J_dict.items():\n",
    "    print(\"Jacobian from {} to logits is\".format(k))\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hessian, with shape (4, 3, 3)\n",
      "[[[ 0.02285465  0.04922541  0.03384247]\n",
      "  [ 0.04922541  0.10602397  0.07289147]\n",
      "  [ 0.03384247  0.07289147  0.05011288]]\n",
      "\n",
      " [[-0.03195215  0.03921401 -0.00544639]\n",
      "  [ 0.03921401 -0.04812629  0.00668421]\n",
      "  [-0.00544639  0.00668421 -0.00092836]]\n",
      "\n",
      " [[-0.01583708 -0.00182736  0.03959271]\n",
      "  [-0.00182736 -0.00021085  0.00456839]\n",
      "  [ 0.03959271  0.00456839 -0.09898177]]\n",
      "\n",
      " [[-0.00103524  0.00348343 -0.00194457]\n",
      "  [ 0.00348343 -0.01172127  0.0065432 ]\n",
      "  [-0.00194457  0.0065432  -0.00365263]]]\n"
     ]
    }
   ],
   "source": [
    "def hessian(f):\n",
    "    return jacfwd(jacrev(f))\n",
    "\n",
    "H = hessian(f)(W)\n",
    "print(\"hessian, with shape\", H.shape)\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    f1 = x[0]**3 + x[1]**4 + x[2]**5\n",
    "    f2 = x[0]*x[1] + x[1]*x[2]\n",
    "    return jnp.array([f1, f2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([3., 2.], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = jnp.array([1., 1., 1.])\n",
    "f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[3., 4., 5.],\n",
       "       [1., 2., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacfwd(f)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[ 6.,  0.,  0.],\n",
       "        [ 0., 12.,  0.],\n",
       "        [ 0.,  0., 20.]],\n",
       "\n",
       "       [[ 0.,  1.,  0.],\n",
       "        [ 1.,  0.,  1.],\n",
       "        [ 0.,  1.,  0.]]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacfwd(jacrev(f))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W (3,)\n",
      "f (4,)\n",
      "v (3,)\n",
      "y (4,)\n",
      "u (4,)\n"
     ]
    }
   ],
   "source": [
    "from jax import jvp \n",
    "f = lambda W: predict(W, b, inputs)\n",
    "print('W', W.shape)\n",
    "print('f', f(W).shape)\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "v = random.normal(subkey, W.shape)\n",
    "print('v', v.shape)\n",
    "\n",
    "y, u = jvp(f, (W,), (v,))\n",
    "print('y', y.shape)\n",
    "print('u', u.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W (3,)\n",
      "f (4,)\n",
      "u (4,)\n",
      "v (Array([-0.0861915 , -0.07484604,  0.09508413], dtype=float32),)\n"
     ]
    }
   ],
   "source": [
    "from jax import vjp\n",
    "f = lambda W: predict(W, b, inputs)\n",
    "print('W', W.shape)\n",
    "print('f', f(W).shape)\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "\n",
    "y, vjp_fun = vjp(f, W)\n",
    "u = random.normal(subkey, y.shape)\n",
    "print('u', u.shape)\n",
    "\n",
    "v = vjp_fun(u)\n",
    "print('v', v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3. 12.]\n",
      " [27. 48.]]\n",
      "[[ 6. 12.]\n",
      " [18. 24.]]\n"
     ]
    }
   ],
   "source": [
    "from jax import vjp \n",
    "\n",
    "def vgrad(f, x):\n",
    "    y, vjp_fn = vjp(f, x)\n",
    "    print(y)\n",
    "    return vjp_fn(jnp.ones(y.shape))[0]\n",
    "\n",
    "v = jnp.array([[1., 2.], [3.,4.]])\n",
    "print(vgrad(lambda x: 3*x**2, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.31 ms ± 14.5 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit jacfwd(lambda x: 3*x**2)(jnp.ones((2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.61 ms ± 20.9 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit jacrev(lambda x: 3*x**2)(jnp.ones((2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hvp(f, x, v):\n",
    "    return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jvp, grad \n",
    "\n",
    "def hvp(f, primals, tangents):\n",
    "    return jvp(grad(f), primals, tangents)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hessian' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m V \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mnormal(subkey2, (\u001b[39m10\u001b[39m, \u001b[39m10\u001b[39m))\n\u001b[1;32m      8\u001b[0m ans1 \u001b[39m=\u001b[39m hvp(f, (X,), (V,))\n\u001b[0;32m----> 9\u001b[0m ans2 \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mtensordot(hessian(f)(X), V, \u001b[39m2\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(jnp\u001b[39m.\u001b[39mallclose(ans1, ans2, \u001b[39m1e-4\u001b[39m, \u001b[39m1e-4\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hessian' is not defined"
     ]
    }
   ],
   "source": [
    "def f(X):\n",
    "    return jnp.sum(jnp.tanh(X)**2)\n",
    "\n",
    "key, subkey1, subkey2 = random.split(key, 3)\n",
    "X = random.normal(subkey1, (10, 10))\n",
    "V = random.normal(subkey2, (10, 10))\n",
    "\n",
    "ans1 = hvp(f, (X,), (V,))\n",
    "ans2 = jnp.tensordot(hessian(f)(X), V, 2)\n",
    "\n",
    "print(jnp.allclose(ans1, ans2, 1e-4, 1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([6., 2.], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return jnp.sum(x**2)\n",
    "\n",
    "x = jnp.array([3., 1.])\n",
    "f(x)\n",
    "grad(f)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([6., 2.], dtype=float32),)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax import vjp \n",
    "\n",
    "y, vjp_fun = vjp(f, x)\n",
    "vjp_fun(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
